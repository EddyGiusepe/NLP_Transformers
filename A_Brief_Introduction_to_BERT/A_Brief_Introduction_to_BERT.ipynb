{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">A Brief Introduction to BERT</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À medida que aprendemos [o que é um Transformer](https://machinelearningmastery.com/the-transformer-model/) e como podemos [treinar o modelo Transformer](https://machinelearningmastery.com/training-the-transformer-model/), percebemos que é uma ótima ferramenta para fazer um computador entender a **linguagem humana**. No entanto, o `Transformer` foi originalmente projetado como um modelo para traduzir um idioma para outro. Se o redirecionarmos para uma tarefa diferente, provavelmente precisaremos treinar novamente todo o modelo do zero. Dado que o tempo necessário para treinar um modelo de `Transformer` é enorme, gostaríamos de ter uma solução que nos permitisse reutilizar prontamente o Transformer treinado para muitas tarefas diferentes. O `BERT` é um desses modelos. É uma extensão da parte do `encoder` (codificador) de um transformador.\n",
    "\n",
    "Neste script, você aprenderá o que é o `BERT` e descobrirá o que ele pode fazer. Os pontos a estudar são:\n",
    "\n",
    "* O que é um `Bidirectional Encoder Representations de Transformer` (<font color=\"yellow\">BERT</font>)\n",
    "\n",
    "* Como um modelo `BERT` pode ser reutilizado para diferentes propósitos\n",
    "\n",
    "* Como você pode usar um modelo `BERT pré-treinado`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do modelo de Transformer ao BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No modelo do `Transformer`, o Encoder (codificador) e o Decoder (decodificador) são conectados para fazer um `modelo seq2seq` para que você faça uma tradução, como do inglês para o alemão, etc. Lembre-se de que a equação da atenção diz:\n",
    "\n",
    "$$\n",
    "attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V,\n",
    "$$\n",
    "\n",
    "onde $Q$, $K$ e $V$ acima é um vetor EMBEDDING transformado por uma matriz de peso no modelo do Transformer. Treinar um modelo de transformer significa encontrar essas **matrizes de peso**. Uma vez que as matrizes de peso são aprendidas, o transformer se torna um **modelo de linguagem**, o que significa que representa uma maneira de entender a linguagem que você usou para treiná-lo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/162096/3_encoder_decoder_class.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um transformer possui partes codificadoras (`Encoder`) e decodificadoras (`Decoder`). Como o nome indica, o <font color=\"yellow\">codificador</font> transforma frases e parágrafos em um formato interno (`uma matriz numérica`) que entende o contexto, enquanto o <font color=\"yellow\">decodificador</font> faz o inverso. A combinação do codificador e do decodificador permite que um transformer execute tarefas `seq2seq`, como `tradução`. Se você retirar a parte do codificador do Transformer, ela pode lhe dizer algo sobre o contexto, o que pode fazer algo interessante.\n",
    "\n",
    "A `representação do codificador bidirecional do Transformer` (BERT) aproveita o modelo de atenção (Attention Model) para obter uma compreensão mais profunda do contexto da linguagem. <font color=\"orange\">`BERT` é uma pilha de muitos blocos codificadores (Encoder)</font>. O texto de entrada é separado em tokens como no modelo do Transformer, e cada token será transformado em um vetor na saída do BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que o BERT pode fazer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um modelo `BERT` é treinado usando o <font color=\"orange\">modelo de linguagem mascarada (MLM)</font> e <font color=\"yellow\">a previsão da próxima sentença (NSP)</font> simultaneamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://machinelearningmastery.com/wp-content/uploads/2022/10/BERT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada amostra de treinamento para `BERT` é um par de frases (sentences) de um documento. As duas frases podem ser consecutivas no documento ou não. Haverá um token `[CLS]` anexado à primeira frase (para representar a **classe**) e um token `[SEP]` anexado a cada frase (como um **separador**). Em seguida, as duas sentenças serão concatenadas como uma sequência de tokens para se tornar uma amostra de treinamento. Uma pequena porcentagem dos tokens na amostra de treinamento é `mascarada` (*masked*) com um token especial `[MASK]` ou substituída por um token aleatório.\n",
    "\n",
    "Antes de ser alimentado no modelo `BERT`, os tokens na amostra de treinamento serão transformados em `vetores EMBEDDINGS`, com as codificações (encodings) posicionais adicionadas, e particulares ao `BERT`, com `segmentos de Embeddings` adicionados, bem como para marcar se o token é da primeira ou da segunda frase.\n",
    "\n",
    "\n",
    "Cada token de entrada para o modelo `BERT` produzirá um vetor de saída. Em um modelo BERT bem treinado, esperamos:\n",
    "\n",
    "* <font color=\"orange\">saída correspondente ao token mascarado pode revelar qual era o token original</font>\n",
    "\n",
    "* <font color=\"orange\">a saída correspondente ao token `[CLS]` no início pode revelar se as duas sentenças são consecutivas no documento</font>\n",
    "\n",
    "\n",
    "Então, os pesos treinados no modelo `BERT` podem entender bem o contexto da linguagem.\n",
    "\n",
    "\n",
    "Depois de ter esse modelo de BERT, você pode usá-lo para muitas `tarefas de downstream`. <font color=\"yellow\">Por exemplo</font>, adicionando uma camada de `classificação` apropriada em cima de um codificador (`Encoder`) e alimentando apenas uma frase ao modelo em vez de um par, você pode usar o token de classe `[CLS]` como entrada para `classificação de sentimento`. Funciona porque a saída do token de classe é treinada para agregar a atenção para toda a entrada.\n",
    "\n",
    "Outro <font color=\"yellow\">exemplo</font> é tomar uma pergunta como a primeira frase e o texto (por exemplo, um parágrafo) como a segunda frase, então o token de saída da segunda frase pode marcar a posição onde a resposta à pergunta estava. Funciona porque a saída de cada token revela algumas informações sobre esse token no contexto de toda a entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando o modelo BERT pré-treinado para sumarização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um modelo de transformer leva muito tempo para treinar do zero. O modelo `BERT` levaria ainda mais tempo. Mas o objetivo do `BERT` é criar um modelo que possa ser reutilizado para muitas tarefas diferentes.\n",
    "\n",
    "Existem modelos `BERT pré-treinados` que você pode usar prontamente. A seguir, você verá alguns casos de uso. O texto usado no exemplo a seguir é de:\n",
    "\n",
    "* [The Bank of England's imperfect Intervention](https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10?barrier=accesspaylog)\n",
    "\n",
    "\n",
    "Teoricamente, um modelo BERT é um codificador (`Encoder`) que mapeia cada token de entrada para um vetor de saída, que pode ser estendido para uma sequência de tokens de comprimento infinito. **Na prática**, existem limitações impostas na implementação de outros componentes que limitam o tamanho da entrada (**input size**). Principalmente, algumas centenas de tokens devem funcionar, pois nem toda implementação pode receber milhares de tokens de uma só vez. Você pode salvar o artigo inteiro em *article.txt* (uma cópia está disponível [aqui](https://machinelearningmastery.com/wp-content/uploads/2022/10/article.txt)). Caso seu modelo precise de um texto menor, você pode usar apenas alguns parágrafos dele.\n",
    "\n",
    "<font color=\"yellow\">Primeiro</font>, vamos explorar a tarefa de `sumarização`. Usando o `BERT`, a ideia é extrair algumas frases do texto original que representem todo o texto. Você pode ver que essa tarefa é semelhante à previsão da próxima frase, na qual, se receber uma frase e o texto, você deseja classificar se eles estão relacionados.\n",
    "\n",
    "\n",
    "\n",
    "Para fazer isso, você precisa usar o módulo Python `bert-extractive-summarizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É um wrapper para alguns modelos `Hugging Face` para fornecer o pipeline de tarefas de `sumarização`. Hugging Face é uma plataforma que permite publicar modelos de aprendizado de máquina, principalmente em tarefas de `NLP`.\n",
    "\n",
    "Depois de instalar o `bert-extractive-summarizer`, produzir um resumo é apenas algumas linhas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amid the political turmoil of outgoing British Prime Minister Liz Truss’s\\nshort-lived government, the Bank of England has found itself in the\\nfiscal-financial crossfire. Whatever government comes next, it is vital\\nthat the BOE learns the right lessons.\\n\\nOn September 23, Truss’s government announced a large, unfunded\\nfiscal-stimulus package that undermined the BOE’s price-stability mandate\\nand caused government-debt and foreign-exchange markets to malfunction.\\nTo prevent systemically important markets from seizing up, the BOE\\npostponed its planned offloading of government bonds (gilts) and instead\\npurchased more. But these unsterilized (monetized) asset purchases amount\\nto an expansionary monetary policy, which will further frustrate the BOE’s\\nefforts to bring down inflation – unless and until the purchases are\\nreversed.\\n\\nUnfortunately for the BOE, it had no choice. Financial stability is a\\nprecondition for sustainable price stability, so it is a central bank’s\\noverriding concern. When the circumstances demand it, central banks must\\nact as lenders of last resort (LOLR) to preserve or restore funding\\nliquidity for systemically important counterparties. They also must\\nserve as market makers of last resort (MMLR) or buyers of last resort\\n(BOLR) to maintain or restore market liquidity in systemically important\\nfinancial markets.\\n\\nThe BOE got its latest interventions partly right, but it also made\\nmistakes. When market participants in the UK – notably liability-driven\\ninvestment (LDI) funds – panicked, the BOE intervened, on September 28,\\nwith a powerful statement about its willingness and ability to act.\\nPurchases would be carried out “on whatever scale is necessary” to restore\\norderly markets. Moreover, the Treasury would fully – and quite\\nproperly – indemnify the operation for any losses incurred.\\n\\nBut the BOE undermined this show of force by putting a time limit on its\\nemergency intervention. It stated that the planned sales from its Asset\\nPurchase Facility would be delayed from October 3 to October 31, and\\nthat it would purchase up to £5 billion ($5.4 billion, as of September 28)\\nof gilts per day for 13 days. These dates were offered not as mere\\nestimates but as a firm interval.\\n\\nOn October 10, the BOE reported that it had bought only around £5 billion\\nof gilts in total over the course of eight daily auctions. It was a\\ndemonstration of credibility at work. If market participants believe\\nthat the MMLR is willing and able to intervene “on whatever scale is\\nnecessary” to restore order, the actual intervention required may be small.\\nIn the case of the European Central Bank’s Outright Monetary Transactions\\n(launched in 2012), no actual purchases were needed to restore market\\nfunctioning.\\n\\nWith £60 billion of unused capacity in its emergency facility, the BOE\\nthen announced that it would increase the maximum size of the remaining\\nfive auctions to £10 billion each. It also added index-linked gilts to\\nthe program and initiated a temporary LOLR facility, the Temporary\\nExpanded Collateral Repo Facility, to extend further assistance to\\nstressed LDI funds; it eased collateral requirements for its regular\\nIndexed Long-Term Repo operations; and it made additional liquidity\\navailable through a new, permanent Short-Term Repo facility, launched\\nthe week of October 3.\\n\\nAll this made sense. But the BOE’s continued insistence that its emergency\\ngilt-purchase program would end on October 14 was unhelpful, because it\\ncontradicted the assertion that it would carry out purchases on whatever\\nscale it deemed necessary. Moreover, that date was never credible. If\\nbondholders were to start dumping gilts again on October 15 or any later\\ndate, the BOE would be right back to acting as a BOLR.\\n\\nIt is unclear whether the BOE’s emergency gilt purchases were on penalty\\nterms, which are necessary to minimize excessive risk-taking by market\\nparticipants who know that the central bank will intervene should fire\\nsales or other dysfunctional behavior threaten market functioning.\\nPenalty terms that apply when markets are disorderly are an important\\ncomplement to well-designed regulations constraining excessive risk-taking\\nwhen markets are orderly. But it can be difficult for a central bank to\\ndetermine a purchase price that is both materially more attractive to\\nwould-be sellers than the disorderly market price as well as less\\nattractive than the (unobservable) orderly market price.\\n\\nA separate but important issue is that the BOE’s Monetary Policy Committee\\nplayed no meaningful role in creating and implementing the emergency\\nprogram. That is surprising, considering that the new gilt purchases will\\ncomplicate and delay the process of bringing inflation back down to the\\ntarget rate. According to a statement by the BOE’s Deputy Governor for\\nFinancial Stability, Jon Cunliffe, the MPC was merely “informed of the\\nissues in the gilt market and briefed in advance of the operation,\\nincluding its financial-stability rationale and the temporary and targeted\\nnature of the purchases.”\\n\\nBy emphasizing the temporary and targeted nature of the purchases, Cunliffe\\nseemed to be suggesting that they do not constitute monetary policy\\noperations. But, in that case, was the specific emergency program – both\\nthe unsterilized asset purchases and the postponement of the asset\\nsales – formally recommended by the Financial Policy Committee, whose\\nmandate is financial stability? Cunliffe’s statement is ambiguous on this\\nquestion: “The FPC was engaged ahead of [the program’s] launch, recognizing\\nthe risks to UK financial stability from dysfunction in the gilt market.\\nThe FPC recommended that the Bank take action, and welcomed the Bank’s plans\\nfor temporary and targeted purchases in the gilt market on financial\\nstability grounds at an urgent pace.”\\n\\nGiven that these emergency operations have both financial-stability\\nobjectives and monetary-policy implications, they ought to have been\\nproperly approved by all the designated decision-making bodies – the FPC\\nand the MPC. If that was not the case, the program’s legitimacy may be\\nsignificantly undermined.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "text = open(\"article.txt\").read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amid the political turmoil of outgoing British Prime Minister Liz Truss’s\n",
      "short-lived government, the Bank of England has found itself in the\n",
      "fiscal-financial crossfire. Whatever government comes next, it is vital\n",
      "that the BOE learns the right lessons. According to a statement by the BOE’s Deputy Governor for\n",
      "Financial Stability, Jon Cunliffe, the MPC was merely “informed of the\n",
      "issues in the gilt market and briefed in advance of the operation,\n",
      "including its financial-stability rationale and the temporary and targeted\n",
      "nature of the purchases.”\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer('distilbert-base-uncased')\n",
    "\n",
    "result = model(text, num_sentences=3)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é o código completo! \n",
    "\n",
    "Nos bastidores, o `spaCy` foi usado em alguns pré-processamentos e o `Hugging Face` foi usado para lançar o modelo. O modelo utilizado foi denominado `distilbert-base-uncased`. O `DistilBERT` é um modelo `BERT` simplificado que pode rodar mais rápido e usar menos memória. O modelo é `“sem maiúsculas”`, o que significa que as maiúsculas ou minúsculas no texto de entrada são consideradas as mesmas, uma vez que são transformadas em vetores EMBEDDINGS.\n",
    "\n",
    "A saída do modelo do `summarizer` é uma string. Conforme você especificou `num_sentences=3` ao chamar o modelo, o resumo são três frases selecionadas do texto. Essa abordagem é chamada de <font color=\"yellow\">resumo extrativo</font>. A alternativa é um <font color=\"yellow\">resumo abstrativo</font>, no qual o resumo é gerado ao invés de extraído do texto. Isso precisaria de um modelo diferente do `BERT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando o modelo BERT pré-treinado para responder a perguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O outro exemplo de uso do `BERT` é combinar perguntas com respostas. Você entregará a pergunta e o texto ao modelo e procurará a saída do início e do fim da resposta do texto.\n",
    "\n",
    "Um exemplo rápido seria apenas algumas linhas de código como segue, reutilizando o mesmo texto de exemplo do exemplo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.42956647276878357, 'start': 1261, 'end': 1344, 'answer': 'to maintain or restore market liquidity in systemically important\\nfinancial markets'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = open(\"article.txt\").read()\n",
    "\n",
    "question = \"What is BOE doing?\"\n",
    " \n",
    "\n",
    "answering = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "result = answering(question=question, context=text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, `Hugging Face` é usado diretamente. Se você instalou o módulo usado no exemplo anterior, o módulo `Hugging Face Python` é uma dependência que você já instalou. Caso contrário, pode ser necessário instalá-lo com pip:\n",
    "\n",
    "```\n",
    "%pip install transformers\n",
    "```\n",
    "\n",
    "E para realmente usar um `modelo Hugging Face`, você também deve ter o `PyTorch` e o `TensorFlow` instalados:\n",
    "\n",
    "```\n",
    "%pip install torch tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saída do código acima é um `dicionário Python`, como segue:\n",
    "\n",
    "![](./saida_question_answer.png)\n",
    "\n",
    "É aqui que você pode encontrar a resposta (que é uma frase do texto de entrada), bem como a posição inicial e final na ordem do token de onde veio essa resposta. A pontuação pode ser considerada como a pontuação de confiança do modelo de que a resposta pode se encaixar na pergunta.\n",
    "\n",
    "Nos bastidores, o que o modelo fez foi gerar uma pontuação de probabilidade para o melhor início do texto que responde à pergunta, bem como o texto para o melhor final. Em seguida, a resposta é extraída encontrando a localização das maiores probabilidades."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
