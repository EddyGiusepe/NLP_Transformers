{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Deep Learning: NLP com Transformer do Hugging Face</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos NLP de Deep Learning usam quase todos a `arquitetura transformer`. Para se familiarizar com essa arquitetura e assim poder usar ou treinar um modelo NLP, existe um conjunto de bibliotecas criadas pela `Hugging Face`. Como essas se tornaram as bibliotecas de facto mais utilizadas, é aconselhável fazer o curso que ensina como usá-las na prática. Esse curso criado pela Hugging Face é gratuito e online. Este script permite que você descubra em português os pontos-chave dele (`parte 1 do curso`).\n",
    "\n",
    "Além de usar Transformer para NLP, é usado, também, para lidar com tarefas de processamento de fala (`speech processing`) e visão computacional. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*WcA4sdfmhkGSRPKPHqNlyQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um aspecto chave é que você pode usar a [arquitetura completa](https://huggingface.co/course/chapter1/4?fw=pt#general-architecture) ou apenas o codificador (encoder) ou o decodificador (decoder) do modelo Transformer, dependendo do tipo de tarefa que você pretende resolver:\n",
    "\n",
    "\n",
    "* o [codificador](https://huggingface.co/course/chapter1/5?fw=pt) (encoder): esses modelos são frequentemente caracterizados como tendo atenção “bidirecional” e são frequentemente chamados de modelos de auto-encoding models; os modelos codificador são mais adequados para tarefas que exigem uma compreensão da sentença completa, como classificação de sentença, reconhecimento de entidade nomeada (e, mais geralmente, classificação de palavras) e resposta extrativa a perguntas (QA).\n",
    "o decodificador (decoder): em cada etapa, para uma determinada palavra, as camadas de atenção só podem acessar as palavras posicionadas antes dela na frase. Esses modelos são frequentemente chamados de modelos auto-regressive models; esses modelos são mais adequados para tarefas que envolvem geração de texto.\n",
    "o codificador-decodificador (encoder-decoder ou sequence-to-sequence): os modelos de sequência a sequência são mais adequados para tarefas que envolvem a geração de novas frases dependendo de uma determinada entrada, como resumo, tradução ou resposta a perguntas generativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_transformers': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "995997e0fa455d01783bd0f7e00bfa7ac3c67ebd3b417bdb15c32058a2eecebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
