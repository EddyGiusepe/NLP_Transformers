{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Training the Transformer Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reunimos o [modelo Transformer completo](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking) e agora estamos prontos para treiná-lo para tradução automática neural. Usaremos um Dataset de treinamento para essa finalidade, que contém pares de frases curtas em `inglês` e `alemão`. Também revisitaremos o papel do mascaramento no cálculo das métricas de *accuracy* e **Loss** durante o processo de treinamento. \n",
    "\n",
    "Neste script, você aprenderá como treinar o modelo `Transformer` para **tradução automática neural**. Os pontos a estudar são: \n",
    "\n",
    "* Como preparar o conjunto de Dados de treinamento\n",
    "\n",
    "* Como aplicar uma `máscara de preenchimento` (Padding Mask) aos cálculos de Loss e accuracy\n",
    "\n",
    "* Como treinar o modelo Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando o conjunto de dados de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso, você pode consultar um tutorial anterior que aborda o material sobre como [preparar os dados de texto](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/) para treinamento. \n",
    "\n",
    "Você também usará um Dataset que contém pares de frases curtas em `Inglês` e `Alemão`, que você pode [baixar aqui](https://github.com/Rishav09/Neural-Machine-Translation-System/blob/master/english-german-both.pkl). Este Dataset específico já foi limpo removendo caracteres não-imprimíveis e não-alfabéticos e caracteres de pontuação, normalizando ainda mais todos os caracteres Unicode para `ASCII` e **alterando todas as letras maiúsculas para minúsculas**. Portanto, você pode pular a etapa de limpeza, que geralmente faz parte do processo de preparação de dados. No entanto, se você usar um conjunto de dados que não seja facilmente limpo, consulte o [tutorial anterior](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/) para aprender como fazer isso.\n",
    "\n",
    "\n",
    "Vamos prosseguir criando a classe `PrepareDataset` que implementa os seguintes passos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"orange\">Carrega o Dataset de um nome de arquivo especificado.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['i like both', 'ich mag beide'],\n",
       "       ['she misses him', 'er fehlt ihr'],\n",
       "       ['i followed him', 'ich folgte ihm'],\n",
       "       ...,\n",
       "       ['tom is cooking', 'tom kocht'],\n",
       "       ['youre upset', 'sie sind besturzt'],\n",
       "       ['do you see me', 'sehen sie mich']], dtype='<U370')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/Rishav09/Neural-Machine-Translation-System\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "clean_dataset = load(open('/home/eddygiusepe/24_Transformers_NLP/NLP_Transformers/Training_the_Transformer_Model/english-german-both.pkl', 'rb'))\n",
    "clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i like both', 'ich mag beide'], dtype='<U370')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"orange\">Seleciona o número de sentenças a serem usadas no conjunto de dados. Como o conjunto de dados é grande, você reduzirá seu tamanho para limitar o tempo de treinamento. No entanto, você pode explorar usando o conjunto de dados completo como uma extensão deste tutorial.</font>\n",
    "\n",
    "```\n",
    "dataset = clean_dataset[:self.n_sentences, :]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"orange\">Acrescenta tokens de início (`<START>`) e end-of-string - fim de string  (`<EOS>`) a cada sentença. Por exemplo, a frase em inglês, `i like to run`, agora se torna, `<START> i like to run <EOS>`. Isso também se aplica à sua tradução correspondente em Alemão, `ich gehe gerne joggen`, que agora se torna, `<START> ich gehe gerne joggen <EOS>`.</font>\n",
    "\n",
    "```\n",
    "for i in range(dataset[:, 0].size):\n",
    "\tdataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "\tdataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    "```\n",
    "\n",
    "\n",
    "* <font color=\"orange\">Embaralha o conjunto de Dados aleatoriamente.</font>\n",
    "\n",
    "```\n",
    "shuffle(dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"orange\">Divide o conjunto de Dados embaralhado com base em uma proporção predefinida.</font>\n",
    "\n",
    "```\n",
    "train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "```\n",
    "\n",
    "* <font color=\"orange\">Cria e treina um tokenizador nas sequências de texto que serão alimentadas no codificador (Encoder) e localiza o comprimento da sequência mais longa, bem como o tamanho do vocabulário. </font>\n",
    "\n",
    "```\n",
    "enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "```\n",
    "\n",
    "* <font color=\"orange\">Tokeniza as sequências de texto que serão alimentadas no codificador (Encoder), criando um vocabulário de palavras e substituindo cada palavra por seu índice de vocabulário correspondente. Os tokens `<START>` e `<EOS>` também farão parte deste vocabulário. Cada sequência também é preenchida (Padded) até o comprimento máximo da frase. </font>\n",
    "\n",
    "```\n",
    "trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "```\n",
    "\n",
    "* <font color=\"orange\">Cria e treina um tokenizador nas sequências de texto que serão alimentadas no decodificador (Decoder) e encontra o comprimento da sequência mais longa, bem como o tamanho do vocabulário.</font>\n",
    "\n",
    "```\n",
    "dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "```\n",
    "\n",
    "* <font color=\"orange\">Repete um procedimento semelhante de tokenização e preenchimento para as sequências de texto que serão alimentadas no decodificador.</font>\n",
    "\n",
    "```\n",
    "trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "```\n",
    "\n",
    "A listagem completa do código é a seguinte (consulte [este tutorial anterior](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/) para mais detalhes):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences # Isto tambén funciona\n",
    "from keras.utils.data_utils import pad_sequences # Isto funciona sem erro\n",
    "\n",
    "from tensorflow import convert_to_tensor, int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(PrepareDataset, self).__init__(**kwargs)\n",
    "\t\tself.n_sentences = 10000  # Number of sentences to include in the dataset\n",
    "\t\tself.train_split = 0.9  # Ratio of the training data split\n",
    " \n",
    "\t# Fit a tokenizer\n",
    "\tdef create_tokenizer(self, dataset):\n",
    "\t\ttokenizer = Tokenizer()\n",
    "\t\ttokenizer.fit_on_texts(dataset)\n",
    " \n",
    "\t\treturn tokenizer\n",
    " \n",
    "\tdef find_seq_length(self, dataset):\n",
    "\t\treturn max(len(seq.split()) for seq in dataset)\n",
    " \n",
    "\tdef find_vocab_size(self, tokenizer, dataset):\n",
    "\t\ttokenizer.fit_on_texts(dataset)\n",
    " \n",
    "\t\treturn len(tokenizer.word_index) + 1\n",
    " \n",
    "\tdef __call__(self, filename, **kwargs):\n",
    "\t\t# Load a clean dataset\n",
    "\t\tclean_dataset = load(open(filename, 'rb'))\n",
    " \n",
    "\t\t# Reduce dataset size\n",
    "\t\tdataset = clean_dataset[:self.n_sentences, :]\n",
    " \n",
    "\t\t# Include start and end of string tokens\n",
    "\t\tfor i in range(dataset[:, 0].size):\n",
    "\t\t\tdataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "\t\t\tdataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    " \n",
    "\t\t# Random shuffle the dataset\n",
    "\t\tshuffle(dataset)\n",
    " \n",
    "\t\t# Split the dataset\n",
    "\t\ttrain = dataset[:int(self.n_sentences * self.train_split)]\n",
    " \n",
    "\t\t# Prepare tokenizer for the encoder input\n",
    "\t\tenc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "\t\tenc_seq_length = self.find_seq_length(train[:, 0])\n",
    "\t\tenc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    " \n",
    "\t\t# Encode and pad the input sequences\n",
    "\t\ttrainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "\t\ttrainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "\t\ttrainX = convert_to_tensor(trainX, dtype=int64)\n",
    " \n",
    "\t\t# Prepare tokenizer for the decoder input\n",
    "\t\tdec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "\t\tdec_seq_length = self.find_seq_length(train[:, 1])\n",
    "\t\tdec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    " \n",
    "\t\t# Encode and pad the input sequences\n",
    "\t\ttrainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "\t\ttrainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "\t\ttrainY = convert_to_tensor(trainY, dtype=int64)\n",
    " \n",
    "\t\treturn trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar treinando o modelo do Transformer, vamos primeiro dar uma olhada na saída da classe `PrepareDataset` correspondente à primeira frase no Dataset de Treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 20:29:37.187925: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-17 20:29:37.187986: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (eddygiusepe): /proc/driver/nvidia/version does not exist\n",
      "2022-11-17 20:29:37.188751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> tom spoke <EOS> \n",
      " tf.Tensor([  1   4 697   2   0   0   0], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "dataset = PrepareDataset()\n",
    "\n",
    "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    " \n",
    "print(train_orig[0, 0], '\\n', trainX[0, :])\n",
    "\n",
    "\n",
    "# Observação: como o Dataset foi embaralhado aleatoriamente, você provavelmente verá uma saída diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que, originalmente, você tinha uma frase de três palavras (`did tom tell you`) à qual você anexou os tokens de início e fim da string. Em seguida, você começou a vetorizar (você pode notar que os tokens `<START>` e `<EOS>` são atribuídos aos índices de vocabulário $1$ e $2$, respectivamente). O texto vetorizado também foi preenchido (padded) com zeros, de forma que o comprimento do resultado final corresponda ao comprimento máximo da sequência do codificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequence length: 7\n"
     ]
    }
   ],
   "source": [
    "print('Encoder sequence length:', enc_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma, você pode verificar os dados de destino (target) correspondentes que são alimentados no decodificador (Decoder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> tom sprach <EOS> \n",
      " tf.Tensor([  1   5 783   2   0   0   0   0   0   0   0   0], shape=(12,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(train_orig[0, 1], '\\n', trainY[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, o comprimento do resultado final corresponde ao comprimento máximo da sequência do decodificador (Decoder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder sequence length: 12\n"
     ]
    }
   ],
   "source": [
    "print('Decoder sequence length:', dec_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando uma máscara de preenchimento (Padding Mask) aos cálculos da Loss e Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lembre-se](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras) de que a importância de ter um padding mask no codificador (ENCODER) e no decodificador (DECODER) é garantir que os **valores zero** que acabamos de anexar às entradas vetorizadas não sejam processados ​​junto com os valores de entrada reais. \n",
    "\n",
    "Isso também vale para o processo de treinamento, onde uma máscara de preenchimento (padding mask) é necessária para que os valores de preenchimento (padding) zero nos dados de destino não sejam considerados no cálculo da Loss e Accuracy.\n",
    "\n",
    "Vamos dar uma olhada no cálculo da `Loss` primeiro. \n",
    "\n",
    "Isso será calculado usando uma função de `Loss de entropia cruzada categórica esparsa` (**sparse categorical cross-entropy loss**) entre os valores de destino e preditos e subsequentemente multiplicado por uma máscara de preenchimento para que apenas os valores diferentes de zero válidos sejam considerados. A perda retornada é a média dos valores não mascarados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the computation of loss\n",
    "    padding_mask = math.logical_not(equal(target, 0))\n",
    "    padding_mask = cast(padding_mask, float32)\n",
    " \n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n",
    " \n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o cálculo da `accuracy`, os valores `previstos` e `alvo` (target) são primeiro comparados. A saída prevista é um tensor de tamanho (`batch_size`, `dec_seq_length`, `dec_vocab_size`) e contém valores de probabilidade (gerados pela função `softmax` no lado do decodificador (Decoder)) para os tokens na saída. Para poder realizar a comparação com os valores alvo, considera-se apenas cada token com o maior valor de probabilidade, sendo recuperado o seu índice de dicionário através da operação: `argmax(prediction, axis=2)`. Após a aplicação de uma máscara de preenchimento (`Padding Mask`), a accuracy retornada é a média dos valores não mascarados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the computation of accuracy\n",
    "    padding_mask = math.logical_not(math.equal(target, 0))\n",
    " \n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(padding_mask, accuracy)\n",
    " \n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    padding_mask = cast(padding_mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    " \n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o modelo do Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    " \n",
    "\n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1\n",
    "\n",
    "\n",
    "# Observação: \n",
    "# considere apenas duas épocas para limitar o tempo de treinamento. No entanto, você pode explorar o treinamento do modelo mais como uma extensão deste tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você também precisa implementar um `escalonador de taxa de aprendizado` (Learning rate scheduler) que inicialmente aumente a taxa de aprendizado linearmente para os primeiros `warmup_steps` e, em seguida, diminua-a proporcionalmente à raiz quadrada inversa do número da passos (step number). `Vaswani et al.` expresse isso pela seguinte fórmula:\n",
    "\n",
    "$$\n",
    "learning\\_rate = d\\_model^{-0.5}.min(step^{-0.5}, step.warmup\\_steps^{-0.5})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64\n",
    "\n",
    "\n",
    "\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super(LRScheduler, self).__init__(**kwargs)\n",
    " \n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    " \n",
    "    def __call__(self, step_num):\n",
    " \n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    " \n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma instância da classe `LRScheduler` é subsequentemente transmitida como o argumento `learning_rate` do otimizador `Adam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_transformers': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "995997e0fa455d01783bd0f7e00bfa7ac3c67ebd3b417bdb15c32058a2eecebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
