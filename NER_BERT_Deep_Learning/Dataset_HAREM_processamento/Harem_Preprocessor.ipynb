{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Convertendo o Dataset HAREM para json</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este estudo está baseado em dois links de dois maravilhosos cientistas:\n",
    "\n",
    "* [HAREM Datasets Preprocessing - Fabiocapsouza](https://github.com/fabiocapsouza/harem_preprocessing)\n",
    "\n",
    "\n",
    "* [Conversor Dataset Harem para JSON](https://github.com/gdutramartins/po-ner-2-portuguese-ner)\n",
    "\n",
    "* [NER com BERT nos Datasets Harem](https://medium.com/@gdutramartins/named-entity-reconigtion-com-bert-nos-datasets-harem-f81bc38e6971)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste script seguiremos a mesma abordagem desses `iluminados cientistas`, ou seja, vou concatenar as três publicações em apenas um `Dataset` para `treino`, `validação` e `teste`.   \n",
    "\n",
    "O [Harem](https://www.linguateca.pt/HAREM/) foi um dos primeiros datasets disponibilizados para a `língua portuguesa`, fazendo parte dos recursos disponibilizados pela [Linguateca](https://www.linguateca.pt/) para Processamento de Linguagem Natural (`NLP`). O `Harem` possui três edições: Primeira (`2006`), MiniHarem (`2008`), Segundo (`2008`). Este Dataset foi construído concatenando todas as versões do Harem em arquivos `json` com anotações por `span`, totalizando um conjunto de `386` documentos.\n",
    "\n",
    "As entidades mencionadas foram anotadas manualmente por um grupo de colaboradores e os textos foram obtidos de fontes diversas, contendo documentos em `português de Portugal e Brasil`. As entidades mencionadas (categorias) do Harem são as seguintes:\n",
    "\n",
    "\n",
    "* PESSOA\n",
    "\n",
    "* ORGANIZACAO\n",
    "\n",
    "* LOCAL\n",
    "\n",
    "* TEMPO\n",
    "\n",
    "* VALOR\n",
    "\n",
    "* ABSTRACAO\n",
    "\n",
    "* ACONTECIMENTO\n",
    "\n",
    "* COISA\n",
    "\n",
    "* OBRA\n",
    "\n",
    "* OUTRO\n",
    "\n",
    "\n",
    "`Filtramos as cinco primeiros categorias`, ignorando as restantes. O Harem classifica as entidades em dois níveis Categoria e Tipo, mas nosso estudo utilizará somente as cinco primeiras categorias, desconsiderando o subnível tipo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Union\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "\n",
    "from lxml import etree\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "ENTITY = Dict[str, Union[str, int]]\n",
    "DOCUMENT = Dict[str, Union[str, List[ENTITY]]]\n",
    "\n",
    "PUNCTUATION_NEED_SPACE = ['.','!',':',';','?',',']\n",
    "PUNCTUATION_NOT_NEED_SPACE = ['\"','#','$','%','&','\\'','(',')','*','+','-','/','','<','=','>','@','[','\\\\',']','^','_','`','{','|','}','~']\n",
    "\n",
    "\n",
    "GDRIVE_PATH:str = '/home/eddygiusepe/Imagens/Eddy_codigos/NLP_Transformers/NER_BERT_Deep_Learning/Dataset_HAREM_processamento/dataset'\n",
    "DATASET_PRIMEIRO_HAREM_ORIGEM = os.path.join(GDRIVE_PATH, 'CDPrimeiroHAREMprimeiroevento.xml')\n",
    "DATASET_MINI_HAREM_ORIGEM = os.path.join(GDRIVE_PATH, 'CDPrimeiroHAREMMiniHAREM.xml')    \n",
    "DATASET_SEGUNDO_HAREM_ORIGEM = os.path.join(GDRIVE_PATH, 'CDSegundoHAREMReRelEM.xml')    \n",
    "DATASETS_HAREM=[DATASET_PRIMEIRO_HAREM_ORIGEM, DATASET_MINI_HAREM_ORIGEM, DATASET_SEGUNDO_HAREM_ORIGEM]\n",
    "DATASET_OUTPUT_PAHT = os.path.join(GDRIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These utility functions are copied from HuggingFace Transformers Library.\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py\n",
    "\"\"\"\n",
    "import unicodedata\n",
    "\n",
    "# Utility functions below are copied from HugginFace Transformers.\n",
    "def _is_whitespace(char: str) -> bool:\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char: str) -> bool:\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char: str) -> bool:\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_whitespace_or_punctuation(char: str) -> bool:\n",
    "    return _is_whitespace(char) or _is_punctuation(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTIVE_CATEGS = [\n",
    "    'PESSOA',\n",
    "    'ORGANIZACAO',\n",
    "    'LOCAL',\n",
    "    'TEMPO',\n",
    "    'VALOR',\n",
    "]\n",
    "\n",
    "ALL_CATEGS = SELECTIVE_CATEGS + [\n",
    "    'ABSTRACCAO',\n",
    "    'ACONTECIMENTO',\n",
    "    'COISA',\n",
    "    'OBRA',\n",
    "    'OUTRO',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypothesisViolation(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaremConverter:\n",
    "    \"\"\"Converts First HAREM XML format to JSON.\n",
    "    \n",
    "    Args:\n",
    "        selective (bool): turns on selective scenario, where only named\n",
    "            entities of tags PESSOA, ORGANIZACAO, LOCAL, TEMPO and VALOR are\n",
    "            considered. Defaults to False.\n",
    "        alt_strategy (str): the strategy used to select the final alternative\n",
    "            when dealing with ALT tags. One of `most_entities` or\n",
    "            `entity_coverage`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 selective: bool = False,\n",
    "                 alt_strategy: str = 'most_entities'):\n",
    "        if selective:\n",
    "            self._accepted_labels = SELECTIVE_CATEGS\n",
    "        else:\n",
    "            self._accepted_labels = ALL_CATEGS\n",
    "        \n",
    "        strategies = ('most_entities', 'entity_coverage')\n",
    "        if alt_strategy not in strategies:\n",
    "            raise ValueError('`alt_strategy` must be one of {}'.format(strategies))\n",
    "        self.alt_strategy = alt_strategy\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _shift_offset(entity: ENTITY, group_offset: int) -> ENTITY:\n",
    "        \"\"\"Shifts start_offset and end_offset by `group_offset` characters.\"\"\"\n",
    "        entity['start_offset'] += group_offset\n",
    "        entity['end_offset'] += group_offset\n",
    "        return entity\n",
    "    \n",
    "    def _get_label(self, entity: etree._Element) -> Union[str, None]:\n",
    "        \"\"\"Gets the label of an entity considering the label scenario.\n",
    "        In case of ambiguity, returns the first acceptable label or None\n",
    "        if there are no acceptable labels.\"\"\"\n",
    "        categ = entity.attrib.get('CATEG')\n",
    "        if categ is None:\n",
    "            logger.debug('Could not find label of entity with attributes %s',\n",
    "                         dict(entity.attrib))\n",
    "            return None\n",
    "        \n",
    "        labels = [label.strip() for label in categ.split('|')]\n",
    "        for label in labels:\n",
    "            if label in self._accepted_labels:\n",
    "                return label\n",
    "        \n",
    "        logger.debug('Ignoring <EM ID=\"%s\" CATEG=\"%s\">.',\n",
    "                     entity.attrib.get(\"ID\"),\n",
    "                     categ)\n",
    "        return None\n",
    "    \n",
    "    def _convert_entity(self, elem: etree._Element) -> ENTITY:\n",
    "        \"\"\"Convert an <EM/> tag into a dict with the relevant information\n",
    "        considering the label scenario.\"\"\"\n",
    "        entity_text = self._get_clean_text(elem.text.lstrip())\n",
    "        \n",
    "        return {\n",
    "            'entity_id': elem.attrib['ID'],\n",
    "            'text': entity_text,\n",
    "            'label': self._get_label(elem),\n",
    "            'start_offset': 0,\n",
    "            'end_offset': len(entity_text),\n",
    "        }\n",
    "\n",
    "\n",
    "    def _iterate_alt_tag(self, alt_tag: etree._Element\n",
    "                        ) -> Tuple[str, List[ENTITY]]:\n",
    "        \"\"\"Iterate over an ALT tag and return the complete text and all\n",
    "        entities inside it as if it was a single alternative.\"\"\"\n",
    "        text = ''\n",
    "        entities = []\n",
    "\n",
    "        alt_tag_text = self._get_clean_text(alt_tag.text)\n",
    "        if alt_tag_text:\n",
    "            text += alt_tag_text \n",
    "        \n",
    "        for tag in alt_tag:\n",
    "            if tag.tag == 'EM':\n",
    "                entity = self._convert_entity(tag)\n",
    "                if entity['label'] is not None:\n",
    "                    self._shift_offset(entity, len(text))\n",
    "                    entities.append(entity)\n",
    "                text = self.append_text_safe(text, entity['text'])\n",
    "                \n",
    "                tag_tail = self._get_clean_text(tag.tail)\n",
    "                if tag_tail:\n",
    "                    text = self.append_text_safe(text, tag_tail)\n",
    "\n",
    "        return text, entities\n",
    "\n",
    "    def _split_alternatives(self,\n",
    "                            alt_text: str,\n",
    "                            alt_entities: List[ENTITY],\n",
    "                            ) -> Tuple[List[str], List[List[ENTITY]]]:\n",
    "        \"\"\"Given the text of an ALT tag and all entities inside it, divide the\n",
    "        text and entities of the distinct alternatives inside ALT.\n",
    "        \n",
    "        Example of ALT tag:\n",
    "            <ALT>Nomes de Origem|<EM ID=\"2011\" {...}>Nomes de Origem</EM></ALT>\n",
    "            \n",
    "            `alt_text` is \"Nomes de Origem|Nomes de Origem\"\n",
    "            `alt_entities` should be [{\n",
    "                'entity_id': 2011,\n",
    "                'start_offset': 16,\n",
    "                'end_offset': 31,\n",
    "                {...}\n",
    "            }]\n",
    "            Result is:\n",
    "                (['Nomes de Origem', 'Nomes de Origem'],  # Texts\n",
    "                 [\n",
    "                     [],  # No entities for first alternative\n",
    "                     [{\n",
    "                         'entity_id': 2011,\n",
    "                         'text': 'Nomes de Origem',\n",
    "                         'start_offset': 0,\n",
    "                         'end_offset': 15,\n",
    "                         'label': '...',  # label etc\n",
    "                     }]\n",
    "                 ])\n",
    "        \"\"\"\n",
    "        # Split the alternative solutions\n",
    "        alt_texts = alt_text.split('|')\n",
    "        if len(alt_texts) < 2:\n",
    "            raise HypothesisViolation(\n",
    "                \"ALT tag must have at least 2 alternatives.\")\n",
    "        \n",
    "        # Find the char offset of all \"|\" chars\n",
    "        divs = [div.start() for div in re.finditer(r'\\|', alt_text)]\n",
    "        \n",
    "        # Split entities into groups of the distinct alternatives.\n",
    "        # One group will later be selected as the true labels.\n",
    "        groups = []\n",
    "        for _ in range(len(alt_texts)):\n",
    "            groups.append([])\n",
    "        \n",
    "        group_ix = 0\n",
    "        group_start_offset = 0\n",
    "        current_group_end = divs[0]\n",
    "        \n",
    "        for entity in alt_entities:\n",
    "            start = entity['start_offset']\n",
    "            \n",
    "            if start > current_group_end:\n",
    "                # Entity belongs to next alternative\n",
    "                group_ix += 1\n",
    "                group_start_offset = current_group_end + 1\n",
    "\n",
    "                if group_ix < len(divs):\n",
    "                    current_group_end = divs[group_ix]\n",
    "                elif group_ix == len(divs):\n",
    "                    current_group_end = len(alt_text)\n",
    "\n",
    "            # Shift entity to discard the offset due to the text of previous\n",
    "            # alternatives\n",
    "            entity = self._shift_offset(dict(entity), -group_start_offset)\n",
    "            groups[group_ix].append(entity)\n",
    "                \n",
    "        assert len(groups) == len(alt_texts)\n",
    "\n",
    "        return alt_texts, groups\n",
    "        \n",
    "    \n",
    "    def _handle_alt(self, alt_tag: etree._Element) -> Tuple[str, List[ENTITY]]:\n",
    "        \"\"\"Handle ALT tag separating all distinct alternative solutions and\n",
    "        then selecting an alternative using the chosen heuristic.\"\"\"\n",
    "\n",
    "        # Extract complete text and all entities inside ALT\n",
    "        tag_text, entities = self._iterate_alt_tag(alt_tag)\n",
    "        # Divide it into the distinct alternatives\n",
    "        alt_texts, groups = self._split_alternatives(tag_text, entities)\n",
    "        \n",
    "        # Choose one alternative (one of alt_text and one of groups) based on\n",
    "        # the selected ALT strategy\n",
    "        if self.alt_strategy == 'most_entities':\n",
    "            # Choose the first group that have the highest number of accepted\n",
    "            # labels\n",
    "            ents_per_group = [len(group) for group in groups]\n",
    "            assert sum(ents_per_group) == len(entities)\n",
    "            N_max = ents_per_group.index(max(ents_per_group))\n",
    "            chosen_entities = groups[N_max]\n",
    "            group_text = alt_texts[N_max]\n",
    "            if sum(ents_per_group) != ents_per_group[N_max]:\n",
    "                # More than 2 groups with entities\n",
    "                not_chosen = groups[:]\n",
    "                not_chosen.remove(chosen_entities)\n",
    "                logger.debug(\n",
    "                    'Choosing ALT %s over alternatives %s', \n",
    "                    chosen_entities,\n",
    "                    not_chosen)\n",
    "        else:\n",
    "            assert self.alt_strategy == 'entity_coverage'\n",
    "            # Choose the group whose entities cover more text\n",
    "            coverages = [sum(len(ent['text']) for ent in group)\n",
    "                         for group in groups]\n",
    "            N_max = coverages.index(max(coverages))\n",
    "            chosen_entities = groups[N_max]\n",
    "            group_text = alt_texts[N_max]\n",
    "        \n",
    "            if sum(coverages) != coverages[N_max]:\n",
    "                # More than 2 groups with entities\n",
    "                logger.debug('Choosing ALT %s over alternatives %s',\n",
    "                             chosen_entities,\n",
    "                             groups[:].remove(chosen_entities))\n",
    "        \n",
    "        return group_text, chosen_entities\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _avoid_word_agglutination(text: str, insertion: str) -> str:\n",
    "        \"\"\"Conditionally inserts one space at the end of `text` to avoid word\n",
    "        agglutination that would happen by concatenating `text` and `insertion`.\n",
    "        \"\"\"\n",
    "        if not text or not insertion:\n",
    "            return text\n",
    "                \n",
    "        #if not _is_whitespace_or_punctuation(text[-1]) \\\n",
    "        #        and not _is_whitespace_or_punctuation(insertion[0]):\n",
    "        if not _is_whitespace(text[-1]) and \\\n",
    "           not _is_whitespace_or_punctuation(insertion[0]) and \\\n",
    "           not text[-1] in PUNCTUATION_NOT_NEED_SPACE:\n",
    "            text += ' '\n",
    "\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def append_text_safe(text: str, piece: str) -> str:\n",
    "        \"\"\"Appends `piece` to `text`, conditionally inserting a space in between\n",
    "        if directly appending would cause agglutination of the last word of\n",
    "        `text` and first word of `piece`.\"\"\"\n",
    "\n",
    "        if text and len(text) > 0 and piece and len(piece) > 0:\n",
    "            #if not _is_whitespace_or_punctuation(text[-1]) and not _is_whitespace_or_punctuation(piece[0]):\n",
    "            if not _is_whitespace(text[-1]) and \\\n",
    "               not _is_whitespace_or_punctuation(piece[0]) and \\\n",
    "               not text[-1] in PUNCTUATION_NOT_NEED_SPACE:    \n",
    "                \n",
    "                text += ' '\n",
    "        \n",
    "        return text + piece\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_clean_text(text: str) -> str:\n",
    "        \"\"\" Retorna o texto limpo de caracteres indedesejáveis \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        text_ret = text.replace('\\n','')\n",
    "        text_ret = re.sub('\\s+', ' ', text_ret)\n",
    "        text_ret = text_ret.strip()\n",
    "        return text_ret\n",
    "\n",
    "\n",
    "    def _convert_tag(self, tag: etree._Element) -> Tuple[str, List[ENTITY]]:\n",
    "        \"\"\"Convert a tag to a dictionary with all the relevant info,\n",
    "        keeping alignment of extracted entities to the original text.\"\"\"\n",
    "        text = ''\n",
    "        entities = []\n",
    "\n",
    "        if tag.tag == 'EM':\n",
    "            entity = self._convert_entity(tag)\n",
    "            if entity['label'] is not None:\n",
    "                entities.append(entity)\n",
    "            text = entity['text']\n",
    "\n",
    "        elif tag.tag == 'ALT':\n",
    "            alt_text, alt_entities = self._handle_alt(tag)\n",
    "            text = alt_text\n",
    "            entities = alt_entities\n",
    "        \n",
    "        tag_tail = self._get_clean_text(tag.tail)\n",
    "        if tag_tail is not None:\n",
    "            text = self._avoid_word_agglutination(text, tag_tail)\n",
    "            text += tag_tail\n",
    "                \n",
    "        return text, entities\n",
    "\n",
    "\n",
    "    def convert_document(self, doc: etree._Element) -> DOCUMENT:\n",
    "        \"\"\"O primeiro Harem e mini Harem utilizam o texto dentro da tag <DOC>,\n",
    "            já o segundo Harem utiliza tag <P> dentro da tag <DOC>. Precisamos\n",
    "            tratar a profundidade onde o texto se encontra nas diferentes versões \n",
    "            do Harem.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        text = ''\n",
    "        entities = []\n",
    "        \n",
    "        if doc.tag != 'DOC':\n",
    "            raise ValueError(\"`convert_document` expects a DOC tag.\")\n",
    "        \n",
    "        if doc.text is not None:\n",
    "            # Initial text before any tag\n",
    "            text += self._get_clean_text(doc.text)\n",
    "        \n",
    "        for prim_nivel_tag in doc:\n",
    "            if prim_nivel_tag.tag == 'P': #segundo harem com tags <p> \n",
    "                tag_text_p = self._get_clean_text(prim_nivel_tag.text)\n",
    "                if tag_text_p:\n",
    "                    text = self.append_text_safe(text, tag_text_p)\n",
    "                for seg_nivel_tag in prim_nivel_tag:\n",
    "                    text = self._convert_document_parts(tag=seg_nivel_tag, text=text, entities=entities)\n",
    "                #algumas tags <P> podem não terminar com ponto, título por exemplo.\n",
    "                if not _is_punctuation(text[-1]):\n",
    "                    text += '.'\n",
    "            else: #primeiro e mini harem, sem tags <p>, texto direto no <doc> \n",
    "                text = self._convert_document_parts(tag=prim_nivel_tag, text=text, entities=entities)\n",
    "                \n",
    "        return {\n",
    "            'doc_id': doc.attrib['DOCID'],\n",
    "            'doc_text': ''.join(text),\n",
    "            'entities': entities,\n",
    "        }\n",
    "\n",
    "    def _convert_document_parts(self, tag: etree._Element, text: str,entities: List[ENTITY]):\n",
    "        \"\"\" Trata a conversão do formato Harem e MiniHarem (sem <p>) e Segundo Harem (com <p>) \"\"\"\n",
    "        tag_text, tag_entities = self._convert_tag(tag)\n",
    "        text = self._avoid_word_agglutination(text, tag_text)\n",
    "\n",
    "        # Entity start and end offsets are relative to begin of `tag`.\n",
    "        # Shift tag_entities by current doc text length.\n",
    "        for entity in tag_entities:\n",
    "            self._shift_offset(entity, len(text))\n",
    "\n",
    "        # If last character was not a whitespace or punctuation, add space\n",
    "        # to prevent that an entity contains a word only partially\n",
    "        if tag_text:\n",
    "            text = self.append_text_safe(text, tag_text)\n",
    "        \n",
    "        entities.extend(tag_entities)\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def convert_xml(cls, xml: str, **kwargs) -> List[DOCUMENT]:\n",
    "        \"\"\"Read a HAREM XML file and convert it to a JSON list according to the\n",
    "        chosen label scenario and alt resolution strategy.\"\"\"\n",
    "        converter = cls(**kwargs)\n",
    "        tree = etree.parse(xml)\n",
    "        \n",
    "        docs = []\n",
    "        i = 0\n",
    "        for doc in tree.findall('//DOC'):\n",
    "            doc_info = converter.convert_document(doc)\n",
    "            docs.append(doc_info)\n",
    "        return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output file to /home/eddygiusepe/Imagens/Eddy_codigos/NLP_Transformers/NER_BERT_Deep_Learning/Dataset_HAREM_processamento/dataset/CDPrimeiroHAREMprimeiroevento.json\n",
      "Writing output file to /home/eddygiusepe/Imagens/Eddy_codigos/NLP_Transformers/NER_BERT_Deep_Learning/Dataset_HAREM_processamento/dataset/CDPrimeiroHAREMMiniHAREM.json\n",
      "Writing output file to /home/eddygiusepe/Imagens/Eddy_codigos/NLP_Transformers/NER_BERT_Deep_Learning/Dataset_HAREM_processamento/dataset/CDSegundoHAREMReRelEM.json\n"
     ]
    }
   ],
   "source": [
    "for dsHarem in DATASETS_HAREM:\n",
    "    converted_data = HaremConverter.convert_xml(xml=dsHarem,\n",
    "    selective=True, alt_strategy='most_entities')\n",
    "\n",
    "\n",
    "    output_file = os.path.join(DATASET_OUTPUT_PAHT, dsHarem.split('/')[-1].replace('xml', 'json'))\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "        \n",
    "    print(f'Writing output file to {output_file}')\n",
    "    with open(output_file, 'w', encoding='utf-8') as fd:\n",
    "        json.dump(converted_data, fd, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "744cf54c6e08528d59b17d69482cd9cfecb0ea2489e1abb457edfe7bddc9bd30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
