{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Implementing the Transformer Encoder from Scratch in TensorFlow and Keras</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo visto como implementar a [atenção escalada do produto escalar](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras) e integrá-la dentro da [atenção](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras) multicabeçal do modelo Transformer, vamos progredir um passo adiante em direção à implementação de um modelo Transformer completo aplicando seu `codificador`. Nosso objetivo final continua sendo aplicar o modelo completo ao Processamento de Linguagem Natural (`NLP`).\n",
    "\n",
    "Neste script, aprenderemos a implementar o `codificador Transformer` do zero no `TensorFlow` e no `Keras`. Ao final deste script, saberemos:\n",
    "\n",
    "* As camadas que fazem parte do codificador Transformer.\n",
    "\n",
    "* Como implementar o codificador Transformer do zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando o Transformer Encoder do zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Rede Neural Feed-Forward Totalmente Conectada e a Normalização de Camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar criando classes para as camadas `Feed Forward` e `Add` & `Norm` que são mostradas no diagrama do script anterior.\n",
    "\n",
    "`Vaswani et al.` nos dizem que a rede `feed-forward` totalmente conectada consiste em `duas transformações lineares` com uma ativação `ReLU` entre elas. A primeira transformação linear produz uma saída de dimensionalidade, $d_{ff}= 2048$, enquanto a segunda transformação linear produz uma saída de dimensionalidade, $d_{model}= 512$.\n",
    "\n",
    "Para isso, vamos primeiro criar a classe `FeedForward` que herda da classe `Layer` base no `Keras` e inicializar as camadas `densas` e a `ativação do ReLU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrescentaremos a ela o método de classe, `call()`, que recebe uma entrada e a passa pelas duas camadas totalmente conectadas com ativação `ReLU`, retornando uma saída de dimensionalidade igual a $512$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def call(self, x):\n",
    "    # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "    x_fc1 = self.fully_connected1(x)\n",
    " \n",
    "    return self.fully_connected2(self.activation(x_fc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é criar outra classe, `AddNormalization`, que também herda da classe `Layer` base no Keras e inicializar uma camada de normalização de Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nela, inclua o seguinte método de classe que soma as entradas e saídas de sua subcamada, que recebe como entradas, e aplica a normalização da camada ao resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def call(self, x, sublayer_x):\n",
    "    # The sublayer input and output need to be of the same shape to be summed\n",
    "    add = x + sublayer_x\n",
    " \n",
    "    # Apply layer normalization to the sum\n",
    "    return self.layer_norm(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Camada do Codificador (The Encoder Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, você implementará a camada do codificador (`Encoder`), que o codificador do Transformer replicará de forma idêntica $N$ vezes. \n",
    "\n",
    "Para isso, vamos criar a classe, `EncoderLayer`, e inicializar todas as sub-camadas que a compõem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, você pode perceber que inicializou instâncias das classes `FeedForward` e `AddNormalization`, que acabou de criar na seção anterior, e atribuiu sua saída às respectivas variáveis, `feed_forward` e `add_norm(1 and 2)`. A camada `Dropout` é autoexplicativa, onde `rate` define a frequência na qual as unidades de entrada são definidas como $0$. Você criou a classe `MultiHeadAttention` em um [tutorial anterior](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras) e, se salvou o código em um script Python separado, não se esqueça `import` ele. Salve ele em um script Python chamado `multihead_attention.py` e, por esse motivo, preciso incluir a linha de código de `multihead_attention` import `MultiHeadAttention`.\n",
    "\n",
    "Vamos agora criar o método de classe, `call()`, que implementa todas as subcamadas do codificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def call(self, x, padding_mask, training):\n",
    "    # Multi-head attention layer\n",
    "    multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "    # Add in a dropout layer\n",
    "    multihead_output = self.dropout1(multihead_output, training=training)\n",
    " \n",
    "    # Followed by an Add & Norm layer\n",
    "    addnorm_output = self.add_norm1(x, multihead_output)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "    # Followed by a fully connected layer\n",
    "    feedforward_output = self.feed_forward(addnorm_output)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "    # Add in another dropout layer\n",
    "    feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    " \n",
    "    # Followed by another Add & Norm layer\n",
    "    return self.add_norm2(addnorm_output, feedforward_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além dos dados de entrada, o método `call()` também pode receber uma máscara de preenchimento (padding mask). Como um breve lembrete do que foi dito em um [tutorial anterior](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras), a máscara de preenchimento (padding mask) é necessária para impedir que o preenchimento de zero na sequência de entrada seja processado junto com os valores de entrada reais. \n",
    "\n",
    "O mesmo método de classe pode receber um  sinalizador `training` que, quando definido como `True`, só aplicará as camadas de Dropout durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O codificador do transformador (The Transformer Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A última etapa é criar uma classe para o `codificador do Transformer`, que deve se chamar `Encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O codificador do Transformer recebe uma sequência de entrada depois que ela passou por um processo de Embedding de palavras e codificação posicional. Para calcular a codificação posicional, vamos usar a classe  `PositionEmbeddingFixedWeights` descrita por Mehreen Saeed [neste tutorial](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/). \n",
    "\n",
    "Como você fez da mesma forma nas seções anteriores, aqui, você também criará um método de classe, `call()`, que aplica `word Embedding` e a codificação posicional à sequência de entrada e alimenta o resulta para $N$ camadas do codificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def call(self, input_sentence, padding_mask, training):\n",
    "    # Generate the positional encoding\n",
    "    pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "    # Add in a dropout layer\n",
    "    x = self.dropout(pos_encoding_output, training=training)\n",
    " \n",
    "    # Pass on the positional encoded values to each encoder layer\n",
    "    for i, layer in enumerate(self.encoder_layer):\n",
    "        x = layer(x, padding_mask, training)\n",
    " \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A listagem de código para o `codificador Transformer` completo é a seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 00:31:38.449926: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 00:31:38.701177: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-23 00:31:38.756907: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 00:31:39.984973: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 00:31:39.985105: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 00:31:39.985113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from positional_encoding import PositionEmbeddingFixedWeights\n",
    " \n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    " \n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    " \n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    " \n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    " \n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    " \n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    " \n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    " \n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    " \n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    " \n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    " \n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    " \n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    " \n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando o código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você trabalhará com os valores de parâmetros especificados no artigo [Attention Is All You Need](https://arxiv.org/abs/1706.03762), de `Vaswani et al. (2017)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    " \n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto à sequência de entrada, você trabalhará com dados fictícios (dummy) por enquanto até chegar ao estágio de [Treinamento do modelo Transformer completo](https://machinelearningmastery.com/training-the-transformer-model) em um tutorial separado, momento em que você usará frases (setences) reais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    " \n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, você criará uma nova instância da classe `Encoder`, atribuindo sua saída à variável `encoder`, subsequentemente alimentando os argumentos de entrada e imprimindo o resultado. Você definirá o argumento de máscara de preenchimento `None` por enquanto, mas retornará a ele quando implementar o `modelo Transformer completo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Juntar tudo produz a seguinte listagem de código:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.37026757  0.34895554  1.797568   ... -0.76706195 -0.28200462\n",
      "    0.84410375]\n",
      "  [ 0.7366137  -0.16808432  1.4786632  ... -0.879607   -0.58513445\n",
      "    1.4018302 ]\n",
      "  [-0.79449517 -0.03571527  1.2775862  ...  0.25089097 -0.40966454\n",
      "    1.5021054 ]\n",
      "  [-0.94086015 -0.55134034  0.8518615  ... -0.54215664 -0.46473953\n",
      "    1.1561706 ]\n",
      "  [-0.43845513 -0.06749729  0.74106365 ...  0.02672507 -0.38886154\n",
      "    0.82621366]]\n",
      "\n",
      " [[-0.114237    0.43645588  1.1501135  ... -0.29895872 -0.02372005\n",
      "    0.7629448 ]\n",
      "  [-0.3223182  -0.5568473   1.8577188  ...  0.35099304 -0.26219663\n",
      "    1.576568  ]\n",
      "  [-1.3828976  -0.34884807  1.7991527  ...  0.1850249  -0.676788\n",
      "    1.2908013 ]\n",
      "  [-0.30661488 -1.0183655   2.0475793  ... -0.9062627  -0.61956036\n",
      "    0.6877462 ]\n",
      "  [ 0.28439015 -0.79692     1.3233557  ...  0.03901152 -0.04809393\n",
      "    1.043814  ]]\n",
      "\n",
      " [[ 0.7505544  -0.86961806  1.3310057  ... -0.03101212 -1.3336269\n",
      "    2.1856549 ]\n",
      "  [ 0.3137339  -0.72647506  1.9560454  ... -0.6425798  -0.8615262\n",
      "    1.46557   ]\n",
      "  [ 0.15948811 -1.5051161   1.8702532  ...  0.41434374 -1.2144316\n",
      "    1.8408098 ]\n",
      "  [ 0.3275824  -1.2183192   2.2447987  ...  0.6043874  -1.5029099\n",
      "    1.441515  ]\n",
      "  [-0.8679771  -0.18020149  1.9636567  ...  0.5635864  -0.9185279\n",
      "    1.246418  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.2565529   0.09610139  1.5724058  ...  0.63440955 -0.75470966\n",
      "    0.66722363]\n",
      "  [ 1.0557207  -0.61983544  1.4979916  ... -0.22091983 -1.064422\n",
      "    0.7181026 ]\n",
      "  [-0.1118602  -0.56496775  1.2037268  ...  0.74841565 -0.7735755\n",
      "    1.3080595 ]\n",
      "  [ 0.31847546 -0.10339784  1.7590928  ...  1.1765305  -0.73921984\n",
      "    0.8165103 ]\n",
      "  [ 0.08359067 -1.3790363   0.957794   ...  0.49901268 -0.04225953\n",
      "    0.6765851 ]]\n",
      "\n",
      " [[ 0.08057184 -0.906558    1.7467008  ... -0.00948043 -1.4341103\n",
      "   -0.3058007 ]\n",
      "  [ 0.8561901  -0.81851476  2.5291193  ... -0.2446794  -0.86315006\n",
      "    0.6349091 ]\n",
      "  [-0.45185974 -0.8600159   1.7941142  ... -0.72301126 -1.4650558\n",
      "    0.6459839 ]\n",
      "  [-0.4489798  -0.06199934  2.4535954  ...  0.85772824 -1.3104647\n",
      "    0.40565526]\n",
      "  [-0.3269839   0.10322285  1.632621   ... -0.2740744  -0.770301\n",
      "    0.1722489 ]]\n",
      "\n",
      " [[-0.9253345  -0.37223053  0.8342591  ... -0.13736594 -1.2366874\n",
      "    0.15981507]\n",
      "  [-0.7196298  -0.02384337  1.0943807  ... -0.08394018 -1.2496314\n",
      "   -0.49227032]\n",
      "  [-0.37596372 -0.6200214   1.859547   ... -0.06266575 -1.2528017\n",
      "    0.7544414 ]\n",
      "  [-0.44681513 -0.69124806  2.1586812  ... -0.09343015 -0.8167917\n",
      "    0.5762072 ]\n",
      "  [-0.5062242  -1.0648004   1.55901    ... -0.49783966 -0.8087513\n",
      "    0.80839646]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    " \n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    " \n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    " \n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    " \n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução desse código produz uma saída de forma `(batch size, sequence length, model dimensionality)`. Observe que você provavelmente verá uma saída diferente devido à inicialização aleatória da sequência de entrada e aos valores dos parâmetros das camadas densas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_transformers': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "995997e0fa455d01783bd0f7e00bfa7ac3c67ebd3b417bdb15c32058a2eecebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
