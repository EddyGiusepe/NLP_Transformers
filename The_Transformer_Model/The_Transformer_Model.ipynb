{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">The Transformer Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui estudaremos os maravilhosos tutoriais de [Machine Learning Mastery - PhD.: Jason Brownlee](https://machinelearningmastery.com/). Aqui em específico começaremos com os seguintes:\n",
    "\n",
    "* [The Transformer Model](https://machinelearningmastery.com/the-transformer-model/)\n",
    "\n",
    "* [Implementing the Transformer Encoder from Scratch in TensorFlow and Keras](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras)\n",
    "\n",
    "* [Implementing the Transformer Decoder from Scratch in TensorFlow and Keras](https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras)\n",
    "\n",
    "* [Joining the Transformer Encoder and Decoder Plus Masking](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking)\n",
    "\n",
    "* [Training the Transformer Model](https://machinelearningmastery.com/training-the-transformer-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já nos familiarizamos com o conceito de `self-attention` (auto-atenção) conforme implementado pelo mecanismo de atenção Transformer para tradução automática neural. Agora vamos mudar nosso foco para os detalhes da **própria arquitetura do Transformer** para descobrir como a auto-atenção pode ser implementada sem depender do uso de `recorrência` e `convoluções`.\n",
    "\n",
    "Aqui aprenderemos sobre a arquitetura de rede do modelo `Transformer`. Os pontos específicos são:\n",
    "\n",
    "\n",
    "* Como a arquitetura Transformer implementa uma estrutura codificador-decodificador sem `recorrência` e `convoluções`\n",
    "\n",
    "* Como funcionam o codificador e o decodificador Transformer \n",
    "\n",
    "* Como a autoatenção do Transformer se compara ao uso de camadas recorrentes e convolucionais \n",
    "\n",
    "\n",
    "Antes de abordar a leitura deste script, recomendamos ler, também, os seguintes tutoriais:\n",
    "\n",
    "* [O conceito de Attention](https://machinelearningmastery.com/what-is-attention/)\n",
    "\n",
    "* [O mecanismo de Attention](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)\n",
    "\n",
    "* [O mecanismo de Attention do Transformer](https://machinelearningmastery.com/the-transformer-attention-mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A arquitetura do Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A arquitetura do Transformer segue uma estrutura de codificador-decodificador, mas não depende de recorrência e convoluções para gerar uma saída. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumindo, a tarefa do codificador, na metade esquerda da arquitetura do Transformer, é mapear uma sequência de entrada para uma sequência de representações contínuas, que é então alimentada em um decodificador. \n",
    "\n",
    "O decodificador, na metade direita da arquitetura, recebe a saída do codificador junto com a saída do decodificador no intervalo de tempo anterior para gerar uma sequência de saída.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_transformers': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "995997e0fa455d01783bd0f7e00bfa7ac3c67ebd3b417bdb15c32058a2eecebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
