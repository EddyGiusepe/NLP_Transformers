{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">[Fine Tune] Fine Tuning BERT for Question Answering (QA) Task</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Question Answering` (QA) é um tipo de tarefa de processamento de linguagem natural em que um modelo é treinado para responder a perguntas com base em um determinado contexto ou passagem de texto. O modelo `BERT` do Google é uma rede neural baseada em `Transformers` pré-treinada que se mostrou eficaz em muitas tarefas de processamento de linguagem natural, incluindo controle de qualidade. Para melhorar o desempenho do `BERT` em uma tarefa de controle de qualidade, o modelo pode ser ajustado em um conjunto de dados menor específico para essa tarefa. Isso permite que o modelo aprenda recursos específicos da tarefa e melhore seu desempenho."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!nvidia-smi\n",
    "\n",
    "# Verificamos CUDA com o Pytorch:\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão leve:\n",
    "#!pip install transformers\n",
    "\n",
    "# Versão de Desenvolvimento:\n",
    "#%pip install transformers[sentencepiece]\n",
    "# !pip install transformers\n",
    "\n",
    "%pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c3ba47b7a045f4917e6b1f2c36f732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login \n",
    "\n",
    "notebook_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionamos nosso e-mail, também, para que se vaja COMMITANDO automaticamente todo o progresso do Treinamento\n",
    "\n",
    "!git config --global user.email \"eddychirinos.unac@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: git-lfs in /home/eddygiusepe/1_Eddy_Giusepe/NLP_Transformers/venv_transformers/lib/python3.10/site-packages (1.6)\n"
     ]
    }
   ],
   "source": [
    "# Devido ao tamanho do que podem ocupar estes modelos\n",
    "\n",
    "%pip install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset \n",
    "\n",
    "squad = load_dataset( \"squad\" , split= \"train[:5000]\" ) \n",
    "squad = squad.train_test_split(test_size= 0.2 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder usar nosso conjunto de dados personalizado no futuro, convertemos o conjunto de dados do formato de `hub HuggingFace` em um `Pandas DataFrame tradicional`. Isso nos ajudará a entender como importar nosso conjunto de dados personalizado no futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56bec29b3aeaaa14008c9380</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>In 2006, Beyoncé introduced her all-female tou...</td>\n",
       "      <td>Beyonce had singers in the background known by...</td>\n",
       "      <td>{'text': ['The Mamas'], 'answer_start': [216]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56cfde92234ae51400d9bfb7</td>\n",
       "      <td>Frédéric_Chopin</td>\n",
       "      <td>Although this period had been productive, the ...</td>\n",
       "      <td>Who did Sand sell Chopin's piano to?</td>\n",
       "      <td>{'text': ['the Canuts'], 'answer_start': [228]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56cd4a5162d2951400fa6517</td>\n",
       "      <td>Sino-Tibetan_relations_during_the_Ming_dynasty</td>\n",
       "      <td>According to Tibetologist John Powers, Tibetan...</td>\n",
       "      <td>Who did the Ming emperors send invitations to?</td>\n",
       "      <td>{'text': ['ruling lamas'], 'answer_start': [416]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56ce345caab44d1400b88581</td>\n",
       "      <td>New_York_City</td>\n",
       "      <td>The first documented visit by a European was i...</td>\n",
       "      <td>What was the name of the first European who ar...</td>\n",
       "      <td>{'text': ['Giovanni da Verrazzano'], 'answer_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56beb2a43aeaaa14008c923a</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Following the death of Freddie Gray, Beyoncé a...</td>\n",
       "      <td>Beyonce with Jay Z gave lots of money to bail ...</td>\n",
       "      <td>{'text': ['protesters'], 'answer_start': [132]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                                           title  \\\n",
       "0  56bec29b3aeaaa14008c9380                                         Beyoncé   \n",
       "1  56cfde92234ae51400d9bfb7                                 Frédéric_Chopin   \n",
       "2  56cd4a5162d2951400fa6517  Sino-Tibetan_relations_during_the_Ming_dynasty   \n",
       "3  56ce345caab44d1400b88581                                   New_York_City   \n",
       "4  56beb2a43aeaaa14008c923a                                         Beyoncé   \n",
       "\n",
       "                                             context  \\\n",
       "0  In 2006, Beyoncé introduced her all-female tou...   \n",
       "1  Although this period had been productive, the ...   \n",
       "2  According to Tibetologist John Powers, Tibetan...   \n",
       "3  The first documented visit by a European was i...   \n",
       "4  Following the death of Freddie Gray, Beyoncé a...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Beyonce had singers in the background known by...   \n",
       "1               Who did Sand sell Chopin's piano to?   \n",
       "2     Who did the Ming emperors send invitations to?   \n",
       "3  What was the name of the first European who ar...   \n",
       "4  Beyonce with Jay Z gave lots of money to bail ...   \n",
       "\n",
       "                                             answers  \n",
       "0     {'text': ['The Mamas'], 'answer_start': [216]}  \n",
       "1    {'text': ['the Canuts'], 'answer_start': [228]}  \n",
       "2  {'text': ['ruling lamas'], 'answer_start': [416]}  \n",
       "3  {'text': ['Giovanni da Verrazzano'], 'answer_s...  \n",
       "4    {'text': ['protesters'], 'answer_start': [132]}  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converte o conjunto de dados em um dicionário\n",
    "data_dict = squad[ \"train\" ].to_dict()\n",
    "\n",
    "# Cria um DataFrame do dicionário\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento no Dataset de Treinamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código, abaixo, está fazendo o pré-processamento em um conjunto de dados armazenado em um DataFrame chamado `“df”`. `Primeiro`, ele está usando o `AutoTokenizer` da biblioteca `Transformers` para tokenizar as perguntas e o contexto do DataFrame. Ele está passando os parâmetros `questions`, `context`, `max_length` e outros para a função tokenizadora. A função tokenizer retorna um dicionário contendo a entrada tokenizada, que é armazenada na variável inputs. O código também recupera o `offset_mapping` das entradas, que é usado para calcular as posições inicial e final das respostas no contexto. Essas posições são armazenadas nas listas `start_positions` e `end_positions`, que são adicionadas como colunas ao DataFrame.\n",
    "\n",
    "Depois disso, ele cria um novo DataFrame a partir dos dados de entrada e das listas `start_positions` e `end_positions`, em seguida, salva o DataFrame em um arquivo chamado CSV `'encoding_train.csv'`. Ele também está convertendo o DataFrame em um Pandas DataFrame tradicional e carregando-o como um conjunto de dados de Train. Esta etapa de pré-processamento está preparando os dados de entrada e as posições de resposta para treinar um modelo de resposta a perguntas. Ao tokenizar a entrada, é possível que o modelo processe a entrada e, ao calcular as posições inicial e final das respostas no contexto, é possível que o modelo aprenda a encontrar as respostas na entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "questions = [q.strip() for q in df[\"question\"]]\n",
    "context = [q.strip() for q in df[\"context\"]]\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "answers = df['answers']\n",
    "for i, offset in enumerate(offset_mapping):\n",
    "    answer = answers[i]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label it (0, 0)\n",
    "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "df[\"start_positions\"] = start_positions\n",
    "df[\"end_positions\"] = end_positions\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "data = {'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'start_positions':start_positions,\n",
    "        'end_positions': end_positions,\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('encoding_train.csv',index=False)\n",
    "train = Dataset.from_pandas(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento no Dataset de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Convert the dataset to a dictionary\n",
    "data_dict = squad[\"test\"].to_dict()\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "questions = [q.strip() for q in df[\"question\"]]\n",
    "context = [q.strip() for q in df[\"context\"]]\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "answers = df['answers']\n",
    "for i, offset in enumerate(offset_mapping):\n",
    "    answer = answers[i]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label it (0, 0)\n",
    "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "df[\"start_positions\"] = start_positions\n",
    "df[\"end_positions\"] = end_positions\n",
    "\n",
    "data = {'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'start_positions':start_positions,\n",
    "        'end_positions': end_positions,\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('encoding_test.csv',index=False)\n",
    "test = Dataset.from_pandas(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustando o BERT a nosso conjunto de Dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repara que estamos usando o modelo `distilbert`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_qa_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora, Treinamos:\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste script, orientamos você pelo processo de ajuste fino do BERT para tarefas de controle de qualidade . O ajuste fino do BERT em uma tarefa de controle de qualidade é benéfico porque requer menos dados de treinamento em comparação com o treinamento de um modelo do zero e pode ser facilmente adaptado para novas tarefas de controle de qualidade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d423ef4205839b3e71f80e760729dfeaabac1e93a593ad42b89eaf709d591b56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
