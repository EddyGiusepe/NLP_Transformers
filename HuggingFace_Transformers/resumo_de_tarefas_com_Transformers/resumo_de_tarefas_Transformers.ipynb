{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Summary of the tasks: How to run the models of the Transformers library task by task</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo das tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalamos a Biblioteca Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "%pip install transformers datasets\n",
    "\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta p√°gina mostra os casos de uso mais frequentes ao usar a biblioteca. Os modelos dispon√≠veis permitem diversas configura√ß√µes e uma grande versatilidade nos casos de uso. As mais simples s√£o apresentadas aqui, mostrando o uso para tarefas como `classifica√ß√£o de imagens` (Imagem classification), `resposta a perguntas` (Question Answering), `classifica√ß√£o de sequ√™ncias`, `reconhecimento de entidades nomeadas` (NER) e outras.\n",
    "\n",
    "Esses exemplos utilizam `auto-models`, que s√£o classes que instanciar√£o um modelo de acordo com um determinado `checkpoint`, selecionando automaticamente a arquitetura de modelo correta. Verifique a documenta√ß√£o do [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) para obter mais informa√ß√µes. Sinta-se √† vontade para modificar o c√≥digo para ser mais espec√≠fico e adapt√°-lo ao seu caso de uso espec√≠fico.\n",
    "\n",
    "Para que um modelo tenha um bom desempenho em uma tarefa, ele deve ser carregado a partir de um checkpoint correspondente a essa tarefa. Esses pontos de verifica√ß√£o geralmente s√£o `pr√©-treinados` em um grande corpus de dados e ajustados (`fine-tuned`) para uma tarefa espec√≠fica. Isso significa o seguinte:\n",
    "\n",
    "* Nem todos os modelos foram fine-tuned em todas as tarefas. Se voc√™ deseja fine-tune um modelo em uma tarefa espec√≠fica, pode aproveitar um dos scripts `run_$TASK.py` no diret√≥rio de [exemplos](https://github.com/huggingface/transformers/tree/main/examples).\n",
    "\n",
    "\n",
    "* Modelos ajustados (fine-tuned) foram ajustados em um conjunto de dados espec√≠fico. Este conjunto de dados pode ou n√£o se sobrepor ao seu caso de uso e dom√≠nio. Conforme mencionado anteriormente, voc√™ pode aproveitar os scripts de [exemplos](https://github.com/huggingface/transformers/tree/main/examples) para ajustar seu modelo ou pode criar seu pr√≥prio script de treinamento.\n",
    "\n",
    "\n",
    "Para fazer uma infer√™ncia sobre uma tarefa, diversos mecanismos s√£o disponibilizados pela biblioteca:\n",
    "\n",
    "* `Pipelines`: abstra√ß√µes muito f√°ceis de usar, que requerem apenas duas linhas de c√≥digo.\n",
    "\n",
    "* `Uso direto do modelo`: menos abstra√ß√µes, mas mais flexibilidade e poder por meio de acesso direto a um `tokenizador` (`PyTorch/TensorFlow`) e capacidade total de infer√™ncia.\n",
    "\n",
    "\n",
    "Ambas as abordagens s√£o apresentadas aqui.\n",
    "\n",
    "Todas as tarefas apresentadas aqui utilizam `checkpoint` pr√©-treinados que foram ajustados em tarefas espec√≠ficas. Carregar um ponto de verifica√ß√£o que n√£o foi ajustado em uma tarefa espec√≠fica carregaria apenas as camadas do `Transformer` de base e n√£o o `head adicional` que √© usado para a tarefa, inicializando os pesos desse head aleatoriamente.\n",
    "\n",
    "Isso produziria uma sa√≠da aleat√≥ria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `classifica√ß√£o de sequ√™ncias` √© a tarefa de classificar as sequ√™ncias de acordo com um determinado n√∫mero de classes. <font color=\"yellow\">Um exemplo de classifica√ß√£o de sequ√™ncia</font> √© o conjunto de dados `GLUE`, que √© totalmente baseado nessa tarefa. Se voc√™ deseja ajustar um modelo em uma tarefa de classifica√ß√£o de sequ√™ncia GLUE, pode aproveitar os scripts [run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py), run_tf_glue.py, run_tf_text_classification.py ou [run_xnli.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_xnli.py).\n",
    "\n",
    "Aqui est√° um exemplo de uso de pipelines para fazer `an√°lise de sentimento`: identificar se uma sequ√™ncia √© `positiva` ou `negativa`. Ele aproveita um modelo fine-tuned em `sst2`, que √© uma tarefa `GLUE`.\n",
    "\n",
    "Isso retorna uma Label (`\"POSITIVO\"` ou `\"NEGATIVO\"`) ao lado de uma pontua√ß√£o, como segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9982\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = classifier(\"I love studying Machine Learning, that's why I became a Data Scientist.\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: NEGATIVE, with score: 0.9642\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"I hate people who don't fight for their dreams.\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° um exemplo de como fazer uma classifica√ß√£o de sequ√™ncia usando um modelo para determinar se duas sequ√™ncias s√£o par√°frases uma da outra. O processo √© o seguinte:\n",
    "\n",
    "<font color=\"orange\">1.</font> Instancie um tokenizer e um modelo a partir do nome do `checkpoint`. O modelo √© identificado como um `modelo BERT` e o carrega com os pesos armazenados no checkpoint.\n",
    "\n",
    "<font color=\"orange\">2.</font> Construa uma sequ√™ncia a partir das duas senten√ßas, com os separadores corretos e espec√≠ficos do modelo, `ids` de tipo de token e `attention masks` (que ser√£o criadas automaticamente pelo tokenizador).\n",
    "\n",
    "<font color=\"orange\">3.</font> Passe essa sequ√™ncia pelo modelo para que ela seja classificada em uma das duas classes dispon√≠veis: $0$$ (`n√£o √© uma par√°frase`) e $1$ (√© uma par√°frase).\n",
    "\n",
    "<font color=\"orange\">4.</font> Calcule o `softmax` do resultado para obter probabilidades sobre as classes.\n",
    "\n",
    "<font color=\"orange\">5.</font> Imprima os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A seguir comparamos a sequence_0 e a sequence_2, as quais S√ÉO PAR√ÇFRASES! \n",
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "print(\"A seguir comparamos a sequence_0 e a sequence_2, as quais S√ÉO PAR√ÇFRASES! \")\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert-base-cased-finetuned-mrpc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\n",
    "\n",
    "paraphrase_classification_logits = model(paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\n",
    "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\n",
    "\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrative Question Answering √© a tarefa de `extrair uma resposta` de um texto a partir de uma pergunta. Um exemplo de um conjunto de dados de resposta a perguntas √© o conjunto de dados `SQuAD`, que √© totalmente baseado nessa tarefa. Se desejar ajustar (`fine-tune`) um modelo em uma tarefa SQuAD, voc√™ pode aproveitar os scripts [run_qa.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py) e `run_tf_squad.py`.\n",
    "\n",
    "Aqui est√° um exemplo de uso de `pipelines` para responder a perguntas: extrair uma resposta de um texto dado a uma pergunta. Ele aproveita um modelo `fine-tuned` no SQUAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso retorna uma `resposta extra√≠da do texto`, uma pontua√ß√£o de confian√ßa, juntamente com os valores `\"start\"` e `\"end\"`, que s√£o as posi√ß√µes da resposta extra√≠da no texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6177, start: 34, end: 95\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° um exemplo de resposta a perguntas (`Question Answering`) usando um modelo e um tokenizador. O processo √© o seguinte:\n",
    "\n",
    "\n",
    "<font color=\"orange\">1.</font> Instancie um tokenizador e um modelo a partir do nome do checkpoint. O modelo √© identificado como um `modelo BERT` e o carrega com os pesos armazenados no ponto de verifica√ß√£o.\n",
    "\n",
    "<font color=\"orange\">2.</font> Defina um texto e algumas perguntas.\n",
    "\n",
    "<font color=\"orange\">3.</font> Repita as perguntas e crie uma sequ√™ncia a partir do texto e da pergunta atual, com os separadores corretos espec√≠ficos do modelo, `IDs` de tipo de token e m√°scaras de aten√ß√£o.\n",
    "\n",
    "<font color=\"orange\">4.</font> Passe esta sequ√™ncia pelo modelo. Isso gera uma variedade de pontua√ß√µes em todos os tokens de sequ√™ncia (`pergunta e texto`), para as posi√ß√µes inicial e final.\n",
    "\n",
    "<font color=\"orange\">5.</font> Calcule o `softmax` do resultado para obter probabilidades sobre os tokens.\n",
    "\n",
    "<font color=\"orange\">6.</font> Busque os tokens dos valores de in√≠cio (`start`) e parada (`stop`) identificados, converta esses tokens em uma string.\n",
    "\n",
    "<font color=\"orange\">7.</font> Imprima os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ü§ó Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does ü§ó Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: ü§ó Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ü§ó Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does ü§ó Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: ü§ó Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
    "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling (Modelagem de linguagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem` √© a tarefa de ajustar (fitting) um modelo a um corpus, que pode ser espec√≠fico de um dom√≠nio. Todos os modelos baseados em Transformer populares s√£o treinados usando uma variante de modelagem de linguagem, <font color=\"yellow\">por exemplo:</font> `BERT` com modelagem de linguagem mascarada (Masked), `GPT-2` com modelagem de linguagem causal.\n",
    "\n",
    "A modelagem de linguagem tamb√©m pode ser √∫til fora do pr√©-treinamento, <font color=\"yellow\">por exemplo,</font> para mudar a distribui√ß√£o do modelo para um <font color=\"pink\">dom√≠nio espec√≠fico</font>: usando um modelo de linguagem treinado em um corpus muito grande e, em seguida, ajustando-o (`fine-tuning`) para um `Dataset de not√≠cias` ou `artigos cient√≠ficos`, por exemplo [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Masked Language Modeling (Modelagem de Linguagem Mascarada)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem mascarada` √© a tarefa de mascarar tokens em uma sequ√™ncia com um token de mascaramento e solicitar que o modelo preencha essa m√°scara com um token apropriado. Isso permite que o modelo atenda tanto ao contexto direito (`tokens √† direita da m√°scara`) quanto ao contexto esquerdo (`tokens √† esquerda da m√°scara`). Esse treinamento cria uma base s√≥lida para tarefas posteriores que exigem `contexto bidirecional`, como `SQuAD` (resposta a perguntas, consulte [Lewis, Lui, Goyal et al., parte 4.2](https://arxiv.org/abs/1910.13461)). Se desejar ajustar (`fine-tune`) um modelo em uma tarefa de **modelagem de linguagem mascarada**, voc√™ pode utilizar o script [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py).\n",
    "\n",
    "Aqui est√° um exemplo de uso de pipelines para substituir uma m√°scara (`mask`) de uma sequ√™ncia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1.31G/1.34G [2:10:37<02:37, 168kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso gera as sequ√™ncias com a m√°scara preenchida (`Mask filled`), a pontua√ß√£o de confian√ßa e o `id` do token no vocabul√°rio do tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17927584052085876,\n",
      "  'sequence': 'HuggingFace is creating a tool that the community uses to solve '\n",
      "              'NLP tasks.',\n",
      "  'token': 3944,\n",
      "  'token_str': ' tool'},\n",
      " {'score': 0.11349426209926605,\n",
      "  'sequence': 'HuggingFace is creating a framework that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 7208,\n",
      "  'token_str': ' framework'},\n",
      " {'score': 0.05243551358580589,\n",
      "  'sequence': 'HuggingFace is creating a library that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 5560,\n",
      "  'token_str': ' library'},\n",
      " {'score': 0.03493541106581688,\n",
      "  'sequence': 'HuggingFace is creating a database that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 8503,\n",
      "  'token_str': ' database'},\n",
      " {'score': 0.02860243059694767,\n",
      "  'sequence': 'HuggingFace is creating a prototype that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 17715,\n",
      "  'token_str': ' prototype'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(\n",
    "    unmasker(\n",
    "        f\"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° um exemplo de `modelagem de linguagem mascarada` usando um modelo e um tokenizador. O processo √© o seguinte:\n",
    "\n",
    "<font color=\"yellow\">1.</font> Instancie um `tokenizador` e um `modelo` a partir do nome do checkpoint. O modelo √© identificado como um modelo `DistilBERT` e o carrega com os pesos armazenados no ponto de verifica√ß√£o.\n",
    "\n",
    "<font color=\"yellow\">2.</font> Defina uma sequ√™ncia com um `token mascarado`, colocando o `tokenizer.mask_token` em vez de uma palavra.\n",
    "\n",
    "<font color=\"yellow\">3.</font> Codifique essa sequ√™ncia em uma lista de IDs e encontre a posi√ß√£o do `token mascarado` nessa lista.\n",
    "\n",
    "<font color=\"yellow\">4.</font> Recupere as previs√µes no √≠ndice do token de m√°scara: ``esse tensor tem o mesmo tamanho do vocabul√°rio`` e os valores s√£o as pontua√ß√µes atribu√≠das a cada token. O modelo atribui maior pontua√ß√£o aos tokens que considera prov√°veis ‚Äã‚Äãnaquele contexto.\n",
    "\n",
    "<font color=\"yellow\">5.</font> Recupere os 5 principais tokens usando os m√©todos `topk` do `PyTorch` ou `top_k` do TensorFlow.\n",
    "\n",
    "<font color=\"yellow\">6.</font> Substitua o token de m√°scara pelos tokens e imprima os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso imprime cinco sequ√™ncias, com os $5$ principais tokens previstos pelo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Causal Language Modeling (Modelagem de Linguagem Causal)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem causal` √© a tarefa de prever o token `seguindo uma sequ√™ncia de tokens`. Nesta situa√ß√£o, o modelo atende apenas ao contexto esquerdo (`tokens √† esquerda da m√°scara`). Tal treinamento √© particularmente interessante para `tarefas de gera√ß√£o`. Se desejar ajustar (`fine-tune`) um modelo em uma tarefa de `modelagem de linguagem causal`, voc√™ pode aproveitar o script [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py).\n",
    "\n",
    "Normalmente, o pr√≥ximo token √© previsto pela amostragem dos logits do √∫ltimo estado oculto que o modelo produz a partir da sequ√™ncia de entrada.\n",
    "\n",
    "Aqui est√° um exemplo de como usar o tokenizador e modelo e alavancar o m√©todo [top_k_top_p_filtering()](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.top_k_top_p_filtering) para amostrar o pr√≥ximo token ap√≥s uma sequ√™ncia de entrada de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is based in DUMBO, New York City, and features\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# get logits of last hidden state\n",
    "next_token_logits = model(**inputs).logits[:, -1, :]\n",
    "\n",
    "# filter\n",
    "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "\n",
    "# sample\n",
    "probs = nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "generated = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° um exemplo de como usar o `tokenizador` e `modelo` e alavancar o m√©todo [tf_top_k_top_p_filtering()](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.tf_top_k_top_p_filtering) para amostrar o pr√≥ximo token ap√≥s uma sequ√™ncia de entrada de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is based in DUMBO, New York City, and focuses\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, tf_top_k_top_p_filtering\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# get logits of last hidden state\n",
    "next_token_logits = model(**inputs).logits[:, -1, :]\n",
    "\n",
    "# filter\n",
    "filtered_next_token_logits = tf_top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "\n",
    "# sample\n",
    "next_token = tf.random.categorical(filtered_next_token_logits, dtype=tf.int32, num_samples=1)\n",
    "\n",
    "generated = tf.concat([input_ids, next_token], axis=1)\n",
    "\n",
    "resulting_string = tokenizer.decode(generated.numpy().tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso gera um pr√≥ximo token (espero) coerente seguindo a sequ√™ncia original, que em nosso caso √© a palavra `is` ou `features`.\n",
    "\n",
    "Na pr√≥xima se√ß√£o, mostramos como [generation.GenerationMixin.generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) pode ser usado para `gerar v√°rios tokens at√© um comprimento especificado em vez de um token por vez`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Text Generation (Gera√ß√£o de texto)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_transformers': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "995997e0fa455d01783bd0f7e00bfa7ac3c67ebd3b417bdb15c32058a2eecebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
