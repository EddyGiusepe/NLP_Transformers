{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Summary of the tasks: How to run the models of the Transformers library task by task</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo das tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalamos a Biblioteca Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "%pip install transformers datasets\n",
    "\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta página mostra os casos de uso mais frequentes ao usar a biblioteca. Os modelos disponíveis permitem diversas configurações e uma grande versatilidade nos casos de uso. As mais simples são apresentadas aqui, mostrando o uso para tarefas como `classificação de imagens` (Imagem classification), `resposta a perguntas` (Question Answering), `classificação de sequências`, `reconhecimento de entidades nomeadas` (NER) e outras.\n",
    "\n",
    "Esses exemplos utilizam `auto-models`, que são classes que instanciarão um modelo de acordo com um determinado `checkpoint`, selecionando automaticamente a arquitetura de modelo correta. Verifique a documentação do [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) para obter mais informações. Sinta-se à vontade para modificar o código para ser mais específico e adaptá-lo ao seu caso de uso específico.\n",
    "\n",
    "Para que um modelo tenha um bom desempenho em uma tarefa, ele deve ser carregado a partir de um checkpoint correspondente a essa tarefa. Esses pontos de verificação geralmente são `pré-treinados` em um grande corpus de dados e ajustados (`fine-tuned`) para uma tarefa específica. Isso significa o seguinte:\n",
    "\n",
    "* Nem todos os modelos foram fine-tuned em todas as tarefas. Se você deseja fine-tune um modelo em uma tarefa específica, pode aproveitar um dos scripts `run_$TASK.py` no diretório de [exemplos](https://github.com/huggingface/transformers/tree/main/examples).\n",
    "\n",
    "\n",
    "* Modelos ajustados (fine-tuned) foram ajustados em um conjunto de dados específico. Este conjunto de dados pode ou não se sobrepor ao seu caso de uso e domínio. Conforme mencionado anteriormente, você pode aproveitar os scripts de [exemplos](https://github.com/huggingface/transformers/tree/main/examples) para ajustar seu modelo ou pode criar seu próprio script de treinamento.\n",
    "\n",
    "\n",
    "Para fazer uma inferência sobre uma tarefa, diversos mecanismos são disponibilizados pela biblioteca:\n",
    "\n",
    "* `Pipelines`: abstrações muito fáceis de usar, que requerem apenas duas linhas de código.\n",
    "\n",
    "* `Uso direto do modelo`: menos abstrações, mas mais flexibilidade e poder por meio de acesso direto a um `tokenizador` (`PyTorch/TensorFlow`) e capacidade total de inferência.\n",
    "\n",
    "\n",
    "Ambas as abordagens são apresentadas aqui.\n",
    "\n",
    "Todas as tarefas apresentadas aqui utilizam `checkpoint` pré-treinados que foram ajustados em tarefas específicas. Carregar um ponto de verificação que não foi ajustado em uma tarefa específica carregaria apenas as camadas do `Transformer` de base e não o `head adicional` que é usado para a tarefa, inicializando os pesos desse head aleatoriamente.\n",
    "\n",
    "Isso produziria uma saída aleatória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `classificação de sequências` é a tarefa de classificar as sequências de acordo com um determinado número de classes. <font color=\"yellow\">Um exemplo de classificação de sequência</font> é o conjunto de dados `GLUE`, que é totalmente baseado nessa tarefa. Se você deseja ajustar um modelo em uma tarefa de classificação de sequência GLUE, pode aproveitar os scripts [run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py), run_tf_glue.py, run_tf_text_classification.py ou [run_xnli.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_xnli.py).\n",
    "\n",
    "Aqui está um exemplo de uso de pipelines para fazer `análise de sentimento`: identificar se uma sequência é `positiva` ou `negativa`. Ele aproveita um modelo fine-tuned em `sst2`, que é uma tarefa `GLUE`.\n",
    "\n",
    "Isso retorna uma Label (`\"POSITIVO\"` ou `\"NEGATIVO\"`) ao lado de uma pontuação, como segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9982\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = classifier(\"I love studying Machine Learning, that's why I became a Data Scientist.\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: NEGATIVE, with score: 0.9642\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"I hate people who don't fight for their dreams.\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está um exemplo de como fazer uma classificação de sequência usando um modelo para determinar se duas sequências são paráfrases uma da outra. O processo é o seguinte:\n",
    "\n",
    "<font color=\"orange\">1.</font> Instancie um tokenizer e um modelo a partir do nome do `checkpoint`. O modelo é identificado como um `modelo BERT` e o carrega com os pesos armazenados no checkpoint.\n",
    "\n",
    "<font color=\"orange\">2.</font> Construa uma sequência a partir das duas sentenças, com os separadores corretos e específicos do modelo, `ids` de tipo de token e `attention masks` (que serão criadas automaticamente pelo tokenizador).\n",
    "\n",
    "<font color=\"orange\">3.</font> Passe essa sequência pelo modelo para que ela seja classificada em uma das duas classes disponíveis: $0$$ (`não é uma paráfrase`) e $1$ (é uma paráfrase).\n",
    "\n",
    "<font color=\"orange\">4.</font> Calcule o `softmax` do resultado para obter probabilidades sobre as classes.\n",
    "\n",
    "<font color=\"orange\">5.</font> Imprima os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A seguir comparamos a sequence_0 e a sequence_2, as quais SÃO PARÂFRASES! \n",
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "print(\"A seguir comparamos a sequence_0 e a sequence_2, as quais SÃO PARÂFRASES! \")\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert-base-cased-finetuned-mrpc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\n",
    "\n",
    "paraphrase_classification_logits = model(paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\n",
    "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\n",
    "\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrative Question Answering é a tarefa de `extrair uma resposta` de um texto a partir de uma pergunta. Um exemplo de um conjunto de dados de resposta a perguntas é o conjunto de dados `SQuAD`, que é totalmente baseado nessa tarefa. Se desejar ajustar (`fine-tune`) um modelo em uma tarefa SQuAD, você pode aproveitar os scripts [run_qa.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py) e `run_tf_squad.py`.\n",
    "\n",
    "Aqui está um exemplo de uso de `pipelines` para responder a perguntas: extrair uma resposta de um texto dado a uma pergunta. Ele aproveita um modelo `fine-tuned` no SQUAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso retorna uma `resposta extraída do texto`, uma pontuação de confiança, juntamente com os valores `\"start\"` e `\"end\"`, que são as posições da resposta extraída no texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6177, start: 34, end: 95\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está um exemplo de resposta a perguntas (`Question Answering`) usando um modelo e um tokenizador. O processo é o seguinte:\n",
    "\n",
    "\n",
    "<font color=\"orange\">1.</font> Instancie um tokenizador e um modelo a partir do nome do checkpoint. O modelo é identificado como um `modelo BERT` e o carrega com os pesos armazenados no ponto de verificação.\n",
    "\n",
    "<font color=\"orange\">2.</font> Defina um texto e algumas perguntas.\n",
    "\n",
    "<font color=\"orange\">3.</font> Repita as perguntas e crie uma sequência a partir do texto e da pergunta atual, com os separadores corretos específicos do modelo, `IDs` de tipo de token e máscaras de atenção.\n",
    "\n",
    "<font color=\"orange\">4.</font> Passe esta sequência pelo modelo. Isso gera uma variedade de pontuações em todos os tokens de sequência (`pergunta e texto`), para as posições inicial e final.\n",
    "\n",
    "<font color=\"orange\">5.</font> Calcule o `softmax` do resultado para obter probabilidades sobre os tokens.\n",
    "\n",
    "<font color=\"orange\">6.</font> Busque os tokens dos valores de início (`start`) e parada (`stop`) identificados, converta esses tokens em uma string.\n",
    "\n",
    "<font color=\"orange\">7.</font> Imprima os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in 🤗 Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does 🤗 Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: 🤗 Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in 🤗 Transformers?\",\n",
    "    \"What does 🤗 Transformers provide?\",\n",
    "    \"🤗 Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in 🤗 Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does 🤗 Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: 🤗 Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in 🤗 Transformers?\",\n",
    "    \"What does 🤗 Transformers provide?\",\n",
    "    \"🤗 Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
    "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling (Modelagem de linguagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem` é a tarefa de ajustar (fitting) um modelo a um corpus, que pode ser específico de um domínio. Todos os modelos baseados em Transformer populares são treinados usando uma variante de modelagem de linguagem, <font color=\"yellow\">por exemplo:</font> `BERT` com modelagem de linguagem mascarada (Masked), `GPT-2` com modelagem de linguagem causal.\n",
    "\n",
    "A modelagem de linguagem também pode ser útil fora do pré-treinamento, <font color=\"yellow\">por exemplo,</font> para mudar a distribuição do modelo para um <font color=\"pink\">domínio específico</font>: usando um modelo de linguagem treinado em um corpus muito grande e, em seguida, ajustando-o (`fine-tuning`) para um `Dataset de notícias` ou `artigos científicos`, por exemplo [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Masked Language Modeling (Modelagem de Linguagem Mascarada)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem mascarada` é a tarefa de mascarar tokens em uma sequência com um token de mascaramento e solicitar que o modelo preencha essa máscara com um token apropriado. Isso permite que o modelo atenda tanto ao contexto direito (`tokens à direita da máscara`) quanto ao contexto esquerdo (`tokens à esquerda da máscara`). Esse treinamento cria uma base sólida para tarefas posteriores que exigem `contexto bidirecional`, como `SQuAD` (resposta a perguntas, consulte [Lewis, Lui, Goyal et al., parte 4.2](https://arxiv.org/abs/1910.13461)). Se desejar ajustar (`fine-tune`) um modelo em uma tarefa de **modelagem de linguagem mascarada**, você pode utilizar o script [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py).\n",
    "\n",
    "Aqui está um exemplo de uso de pipelines para substituir uma máscara (`mask`) de uma sequência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading:  98%|█████████▊| 1.31G/1.34G [2:10:37<02:37, 168kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso gera as sequências com a máscara preenchida (`Mask filled`), a pontuação de confiança e o `id` do token no vocabulário do tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17927584052085876,\n",
      "  'sequence': 'HuggingFace is creating a tool that the community uses to solve '\n",
      "              'NLP tasks.',\n",
      "  'token': 3944,\n",
      "  'token_str': ' tool'},\n",
      " {'score': 0.11349426209926605,\n",
      "  'sequence': 'HuggingFace is creating a framework that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 7208,\n",
      "  'token_str': ' framework'},\n",
      " {'score': 0.05243551358580589,\n",
      "  'sequence': 'HuggingFace is creating a library that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 5560,\n",
      "  'token_str': ' library'},\n",
      " {'score': 0.03493541106581688,\n",
      "  'sequence': 'HuggingFace is creating a database that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 8503,\n",
      "  'token_str': ' database'},\n",
      " {'score': 0.02860243059694767,\n",
      "  'sequence': 'HuggingFace is creating a prototype that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 17715,\n",
      "  'token_str': ' prototype'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(\n",
    "    unmasker(\n",
    "        f\"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está um exemplo de `modelagem de linguagem mascarada` usando um modelo e um tokenizador. O processo é o seguinte:\n",
    "\n",
    "<font color=\"yellow\">1.</font> Instancie um `tokenizador` e um `modelo` a partir do nome do checkpoint. O modelo é identificado como um modelo `DistilBERT` e o carrega com os pesos armazenados no ponto de verificação.\n",
    "\n",
    "<font color=\"yellow\">2.</font> Defina uma sequência com um `token mascarado`, colocando o `tokenizer.mask_token` em vez de uma palavra.\n",
    "\n",
    "<font color=\"yellow\">3.</font> Codifique essa sequência em uma lista de IDs e encontre a posição do `token mascarado` nessa lista.\n",
    "\n",
    "<font color=\"yellow\">4.</font> Recupere as previsões no índice do token de máscara: ``esse tensor tem o mesmo tamanho do vocabulário`` e os valores são as pontuações atribuídas a cada token. O modelo atribui maior pontuação aos tokens que considera prováveis ​​naquele contexto.\n",
    "\n",
    "<font color=\"yellow\">5.</font> Recupere os 5 principais tokens usando os métodos `topk` do `PyTorch` ou `top_k` do TensorFlow.\n",
    "\n",
    "<font color=\"yellow\">6.</font> Substitua o token de máscara pelos tokens e imprima os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso imprime cinco sequências, com os $5$ principais tokens previstos pelo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Causal Language Modeling (Modelagem de Linguagem Causal)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem causal` é a tarefa de prever o token `seguindo uma sequência de tokens`. Nesta situação, o modelo atende apenas ao contexto esquerdo (`tokens à esquerda da máscara`). Tal treinamento é particularmente interessante para `tarefas de geração`. Se desejar ajustar (`fine-tune`) um modelo em uma tarefa de `modelagem de linguagem causal`, você pode aproveitar o script [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py).\n",
    "\n",
    "Normalmente, o próximo token é previsto pela amostragem dos logits do último estado oculto que o modelo produz a partir da sequência de entrada.\n",
    "\n",
    "Aqui está um exemplo de como usar o tokenizador e modelo e alavancar o método [top_k_top_p_filtering()](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.top_k_top_p_filtering) para amostrar o próximo token após uma sequência de entrada de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is based in DUMBO, New York City, and features\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# get logits of last hidden state\n",
    "next_token_logits = model(**inputs).logits[:, -1, :]\n",
    "\n",
    "# filter\n",
    "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "\n",
    "# sample\n",
    "probs = nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "generated = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está um exemplo de como usar o `tokenizador` e `modelo` e alavancar o método [tf_top_k_top_p_filtering()](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.tf_top_k_top_p_filtering) para amostrar o próximo token após uma sequência de entrada de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is based in DUMBO, New York City, and focuses\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, tf_top_k_top_p_filtering\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# get logits of last hidden state\n",
    "next_token_logits = model(**inputs).logits[:, -1, :]\n",
    "\n",
    "# filter\n",
    "filtered_next_token_logits = tf_top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "\n",
    "# sample\n",
    "next_token = tf.random.categorical(filtered_next_token_logits, dtype=tf.int32, num_samples=1)\n",
    "\n",
    "generated = tf.concat([input_ids, next_token], axis=1)\n",
    "\n",
    "resulting_string = tokenizer.decode(generated.numpy().tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso gera um próximo token (espero) coerente seguindo a sequência original, que em nosso caso é a palavra `is` ou `features`.\n",
    "\n",
    "Na próxima seção, mostramos como [generation.GenerationMixin.generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) pode ser usado para `gerar vários tokens até um comprimento especificado em vez de um token por vez`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Text Generation (Geração de texto)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_transformers': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "995997e0fa455d01783bd0f7e00bfa7ac3c67ebd3b417bdb15c32058a2eecebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
