{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Summary of the tasks: How to run the models of the Transformers library task by task</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo das tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalamos a Biblioteca Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "%pip install transformers datasets\n",
    "\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta p√°gina mostra os casos de uso mais frequentes ao usar a biblioteca. Os modelos dispon√≠veis permitem diversas configura√ß√µes e uma grande versatilidade nos casos de uso. As mais simples s√£o apresentadas aqui, mostrando o uso para tarefas como `classifica√ß√£o de imagens` (Imagem classification), `resposta a perguntas` (Question Answering), `classifica√ß√£o de sequ√™ncias`, `reconhecimento de entidades nomeadas` (NER) e outras.\n",
    "\n",
    "Esses exemplos utilizam `auto-models`, que s√£o classes que instanciar√£o um modelo de acordo com um determinado `checkpoint`, selecionando automaticamente a arquitetura de modelo correta. Verifique a documenta√ß√£o do [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) para obter mais informa√ß√µes. Sinta-se √† vontade para modificar o c√≥digo para ser mais espec√≠fico e adapt√°-lo ao seu caso de uso espec√≠fico.\n",
    "\n",
    "Para que um modelo tenha um bom desempenho em uma tarefa, ele deve ser carregado a partir de um checkpoint correspondente a essa tarefa. Esses pontos de verifica√ß√£o geralmente s√£o `pr√©-treinados` em um grande corpus de dados e ajustados (`fine-tuned`) para uma tarefa espec√≠fica. Isso significa o seguinte:\n",
    "\n",
    "* Nem todos os modelos foram fine-tuned em todas as tarefas. Se voc√™ deseja fine-tune um modelo em uma tarefa espec√≠fica, pode aproveitar um dos scripts `run_$TASK.py` no diret√≥rio de [exemplos](https://github.com/huggingface/transformers/tree/main/examples).\n",
    "\n",
    "\n",
    "* Modelos ajustados (fine-tuned) foram ajustados em um conjunto de dados espec√≠fico. Este conjunto de dados pode ou n√£o se sobrepor ao seu caso de uso e dom√≠nio. Conforme mencionado anteriormente, voc√™ pode aproveitar os scripts de [exemplos](https://github.com/huggingface/transformers/tree/main/examples) para ajustar seu modelo ou pode criar seu pr√≥prio script de treinamento.\n",
    "\n",
    "\n",
    "Para fazer uma infer√™ncia sobre uma tarefa, diversos mecanismos s√£o disponibilizados pela biblioteca:\n",
    "\n",
    "* `Pipelines`: abstra√ß√µes muito f√°ceis de usar, que requerem apenas duas linhas de c√≥digo.\n",
    "\n",
    "* `Uso direto do modelo`: menos abstra√ß√µes, mas mais flexibilidade e poder por meio de acesso direto a um `tokenizador` (`PyTorch/TensorFlow`) e capacidade total de infer√™ncia.\n",
    "\n",
    "\n",
    "Ambas as abordagens s√£o apresentadas aqui.\n",
    "\n",
    "Todas as tarefas apresentadas aqui utilizam `checkpoint` pr√©-treinados que foram ajustados em tarefas espec√≠ficas. Carregar um ponto de verifica√ß√£o que n√£o foi ajustado em uma tarefa espec√≠fica carregaria apenas as camadas do `Transformer` de base e n√£o o `head adicional` que √© usado para a tarefa, inicializando os pesos desse head aleatoriamente.\n",
    "\n",
    "Isso produziria uma sa√≠da aleat√≥ria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `classifica√ß√£o de sequ√™ncias` √© a tarefa de classificar as sequ√™ncias de acordo com um determinado n√∫mero de classes. <font color=\"yellow\">Um exemplo de classifica√ß√£o de sequ√™ncia</font> √© o conjunto de dados `GLUE`, que √© totalmente baseado nessa tarefa. Se voc√™ deseja ajustar um modelo em uma tarefa de classifica√ß√£o de sequ√™ncia GLUE, pode aproveitar os scripts [run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py), run_tf_glue.py, run_tf_text_classification.py ou [run_xnli.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_xnli.py).\n",
    "\n",
    "Aqui est√° um exemplo de uso de pipelines para fazer `an√°lise de sentimento`: identificar se uma sequ√™ncia √© `positiva` ou `negativa`. Ele aproveita um modelo fine-tuned em `sst2`, que √© uma tarefa `GLUE`.\n",
    "\n",
    "Isso retorna uma Label (`\"POSITIVO\"` ou `\"NEGATIVO\"`) ao lado de uma pontua√ß√£o, como segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9982\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = classifier(\"I love studying Machine Learning, that's why I became a Data Scientist.\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: NEGATIVE, with score: 0.9642\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"I hate people who don't fight for their dreams.\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° um exemplo de como fazer uma classifica√ß√£o de sequ√™ncia usando um modelo para determinar se duas sequ√™ncias s√£o par√°frases uma da outra. O processo √© o seguinte:\n",
    "\n",
    "<font color=\"orange\">1.</font> Instancie um tokenizer e um modelo a partir do nome do `checkpoint`. O modelo √© identificado como um `modelo BERT` e o carrega com os pesos armazenados no checkpoint.\n",
    "\n",
    "<font color=\"orange\">2.</font> Construa uma sequ√™ncia a partir das duas senten√ßas, com os separadores corretos e espec√≠ficos do modelo, `ids` de tipo de token e `attention masks` (que ser√£o criadas automaticamente pelo tokenizador).\n",
    "\n",
    "<font color=\"orange\">3.</font> Passe essa sequ√™ncia pelo modelo para que ela seja classificada em uma das duas classes dispon√≠veis: $0$$ (`n√£o √© uma par√°frase`) e $1$ (√© uma par√°frase).\n",
    "\n",
    "<font color=\"orange\">4.</font> Calcule o `softmax` do resultado para obter probabilidades sobre as classes.\n",
    "\n",
    "<font color=\"orange\">5.</font> Imprima os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A seguir comparamos a sequence_0 e a sequence_2, as quais S√ÉO PAR√ÇFRASES! \n",
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "print(\"A seguir comparamos a sequence_0 e a sequence_2, as quais S√ÉO PAR√ÇFRASES! \")\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert-base-cased-finetuned-mrpc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\n",
    "\n",
    "paraphrase_classification_logits = model(paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\n",
    "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\n",
    "\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrative Question Answering √© a tarefa de `extrair uma resposta` de um texto a partir de uma pergunta. Um exemplo de um conjunto de dados de resposta a perguntas √© o conjunto de dados `SQuAD`, que √© totalmente baseado nessa tarefa. Se desejar ajustar (`fine-tune`) um modelo em uma tarefa SQuAD, voc√™ pode aproveitar os scripts [run_qa.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py) e `run_tf_squad.py`.\n",
    "\n",
    "Aqui est√° um exemplo de uso de `pipelines` para responder a perguntas: extrair uma resposta de um texto dado a uma pergunta. Ele aproveita um modelo `fine-tuned` no SQUAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso retorna uma `resposta extra√≠da do texto`, uma pontua√ß√£o de confian√ßa, juntamente com os valores `\"start\"` e `\"end\"`, que s√£o as posi√ß√µes da resposta extra√≠da no texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6177, start: 34, end: 95\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° um exemplo de resposta a perguntas (`Question Answering`) usando um modelo e um tokenizador. O processo √© o seguinte:\n",
    "\n",
    "\n",
    "<font color=\"orange\">1.</font> Instancie um tokenizador e um modelo a partir do nome do checkpoint. O modelo √© identificado como um `modelo BERT` e o carrega com os pesos armazenados no ponto de verifica√ß√£o.\n",
    "\n",
    "<font color=\"orange\">2.</font> Defina um texto e algumas perguntas.\n",
    "\n",
    "<font color=\"orange\">3.</font> Repita as perguntas e crie uma sequ√™ncia a partir do texto e da pergunta atual, com os separadores corretos espec√≠ficos do modelo, `IDs` de tipo de token e m√°scaras de aten√ß√£o.\n",
    "\n",
    "<font color=\"orange\">4.</font> Passe esta sequ√™ncia pelo modelo. Isso gera uma variedade de pontua√ß√µes em todos os tokens de sequ√™ncia (`pergunta e texto`), para as posi√ß√µes inicial e final.\n",
    "\n",
    "<font color=\"orange\">5.</font> Calcule o `softmax` do resultado para obter probabilidades sobre os tokens.\n",
    "\n",
    "<font color=\"orange\">6.</font> Busque os tokens dos valores de in√≠cio (`start`) e parada (`stop`) identificados, converta esses tokens em uma string.\n",
    "\n",
    "<font color=\"orange\">7.</font> Imprima os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ü§ó Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does ü§ó Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: ü§ó Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ü§ó Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does ü§ó Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: ü§ó Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
    "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling (Modelagem de linguagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem` √© a tarefa de ajustar (fitting) um modelo a um corpus, que pode ser espec√≠fico de um dom√≠nio. Todos os modelos baseados em Transformer populares s√£o treinados usando uma variante de modelagem de linguagem, <font color=\"yellow\">por exemplo:</font> `BERT` com modelagem de linguagem mascarada (Masked), `GPT-2` com modelagem de linguagem causal.\n",
    "\n",
    "A modelagem de linguagem tamb√©m pode ser √∫til fora do pr√©-treinamento, <font color=\"yellow\">por exemplo,</font> para mudar a distribui√ß√£o do modelo para um <font color=\"pink\">dom√≠nio espec√≠fico</font>: usando um modelo de linguagem treinado em um corpus muito grande e, em seguida, ajustando-o (`fine-tuning`) para um `Dataset de not√≠cias` ou `artigos cient√≠ficos`, por exemplo [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Masked Language Modeling (Modelagem de Linguagem Mascarada)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem mascarada` √© a tarefa de mascarar tokens em uma sequ√™ncia com um token de mascaramento e solicitar que o modelo preencha essa m√°scara com um token apropriado. Isso permite que o modelo atenda tanto ao contexto direito (`tokens √† direita da m√°scara`) quanto ao contexto esquerdo (`tokens √† esquerda da m√°scara`). Esse treinamento cria uma base s√≥lida para tarefas posteriores que exigem `contexto bidirecional`, como `SQuAD` (resposta a perguntas, consulte [Lewis, Lui, Goyal et al., parte 4.2](https://arxiv.org/abs/1910.13461)). Se desejar ajustar (`fine-tune`) um modelo em uma tarefa de **modelagem de linguagem mascarada**, voc√™ pode utilizar o script [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py).\n",
    "\n",
    "Aqui est√° um exemplo de uso de pipelines para substituir uma m√°scara (`mask`) de uma sequ√™ncia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1.31G/1.34G [2:10:37<02:37, 168kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso gera as sequ√™ncias com a m√°scara preenchida (`Mask filled`), a pontua√ß√£o de confian√ßa e o `id` do token no vocabul√°rio do tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17927584052085876,\n",
      "  'sequence': 'HuggingFace is creating a tool that the community uses to solve '\n",
      "              'NLP tasks.',\n",
      "  'token': 3944,\n",
      "  'token_str': ' tool'},\n",
      " {'score': 0.11349426209926605,\n",
      "  'sequence': 'HuggingFace is creating a framework that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 7208,\n",
      "  'token_str': ' framework'},\n",
      " {'score': 0.05243551358580589,\n",
      "  'sequence': 'HuggingFace is creating a library that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 5560,\n",
      "  'token_str': ' library'},\n",
      " {'score': 0.03493541106581688,\n",
      "  'sequence': 'HuggingFace is creating a database that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 8503,\n",
      "  'token_str': ' database'},\n",
      " {'score': 0.02860243059694767,\n",
      "  'sequence': 'HuggingFace is creating a prototype that the community uses to '\n",
      "              'solve NLP tasks.',\n",
      "  'token': 17715,\n",
      "  'token_str': ' prototype'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(\n",
    "    unmasker(\n",
    "        f\"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° um exemplo de `modelagem de linguagem mascarada` usando um modelo e um tokenizador. O processo √© o seguinte:\n",
    "\n",
    "<font color=\"yellow\">1.</font> Instancie um `tokenizador` e um `modelo` a partir do nome do checkpoint. O modelo √© identificado como um modelo `DistilBERT` e o carrega com os pesos armazenados no ponto de verifica√ß√£o.\n",
    "\n",
    "<font color=\"yellow\">2.</font> Defina uma sequ√™ncia com um `token mascarado`, colocando o `tokenizer.mask_token` em vez de uma palavra.\n",
    "\n",
    "<font color=\"yellow\">3.</font> Codifique essa sequ√™ncia em uma lista de IDs e encontre a posi√ß√£o do `token mascarado` nessa lista.\n",
    "\n",
    "<font color=\"yellow\">4.</font> Recupere as previs√µes no √≠ndice do token de m√°scara: ``esse tensor tem o mesmo tamanho do vocabul√°rio`` e os valores s√£o as pontua√ß√µes atribu√≠das a cada token. O modelo atribui maior pontua√ß√£o aos tokens que considera prov√°veis ‚Äã‚Äãnaquele contexto.\n",
    "\n",
    "<font color=\"yellow\">5.</font> Recupere os 5 principais tokens usando os m√©todos `topk` do `PyTorch` ou `top_k` do TensorFlow.\n",
    "\n",
    "<font color=\"yellow\">6.</font> Substitua o token de m√°scara pelos tokens e imprima os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso imprime cinco sequ√™ncias, com os $5$ principais tokens previstos pelo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Causal Language Modeling (Modelagem de Linguagem Causal)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `modelagem de linguagem causal` √© a tarefa de prever o token `seguindo uma sequ√™ncia de tokens`. Nesta situa√ß√£o, o modelo atende apenas ao contexto esquerdo (`tokens √† esquerda da m√°scara`). Tal treinamento √© particularmente interessante para `tarefas de gera√ß√£o`. Se desejar ajustar (`fine-tune`) um modelo em uma tarefa de `modelagem de linguagem causal`, voc√™ pode aproveitar o script [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py).\n",
    "\n",
    "Normalmente, o pr√≥ximo token √© previsto pela amostragem dos logits do √∫ltimo estado oculto que o modelo produz a partir da sequ√™ncia de entrada.\n",
    "\n",
    "Aqui est√° um exemplo de como usar o tokenizador e modelo e alavancar o m√©todo [top_k_top_p_filtering()](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.top_k_top_p_filtering) para amostrar o pr√≥ximo token ap√≥s uma sequ√™ncia de entrada de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is based in DUMBO, New York City, and features\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# get logits of last hidden state\n",
    "next_token_logits = model(**inputs).logits[:, -1, :]\n",
    "\n",
    "# filter\n",
    "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "\n",
    "# sample\n",
    "probs = nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "generated = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° um exemplo de como usar o `tokenizador` e `modelo` e alavancar o m√©todo [tf_top_k_top_p_filtering()](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.tf_top_k_top_p_filtering) para amostrar o pr√≥ximo token ap√≥s uma sequ√™ncia de entrada de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is based in DUMBO, New York City, and focuses\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, tf_top_k_top_p_filtering\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# get logits of last hidden state\n",
    "next_token_logits = model(**inputs).logits[:, -1, :]\n",
    "\n",
    "# filter\n",
    "filtered_next_token_logits = tf_top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "\n",
    "# sample\n",
    "next_token = tf.random.categorical(filtered_next_token_logits, dtype=tf.int32, num_samples=1)\n",
    "\n",
    "generated = tf.concat([input_ids, next_token], axis=1)\n",
    "\n",
    "resulting_string = tokenizer.decode(generated.numpy().tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso gera um pr√≥ximo token (espero) coerente seguindo a sequ√™ncia original, que em nosso caso √© a palavra `is` ou `features`.\n",
    "\n",
    "Na pr√≥xima se√ß√£o, mostramos como [generation.GenerationMixin.generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) pode ser usado para `gerar v√°rios tokens at√© um comprimento especificado em vez de um token por vez`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Text Generation (Gera√ß√£o de texto)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na `gera√ß√£o de texto` (tamb√©m conhecido `como gera√ß√£o de texto aberto`), o objetivo √© criar uma parte coerente do texto que seja uma continua√ß√£o do contexto dado. O exemplo a seguir mostra como `GPT-2` pode ser usado em pipelines para gerar texto. Como padr√£o, todos os modelos aplicam a amostragem `Top-K` quando usados ‚Äã‚Äãem pipelines, conforme configurado em suas respectivas configura√ß√µes (consulte a [gpt-2 config](https://huggingface.co/gpt2/blob/main/config.json), por exemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, o modelo gera um texto aleat√≥rio com um `comprimento m√°ximo total de $50$ tokens` do contexto \"As far as I am concerned, I will\". Nos bastidores, o objeto pipeline chama o m√©todo [PreTrainedModel.generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) para gerar texto. Os argumentos padr√£o para esse m√©todo podem ser substitu√≠dos no pipeline, conforme mostrado acima para os argumentos `max_length` e `do_sample`.\n",
    "\n",
    "Segue abaixo um exemplo de `gera√ß√£o de texto` utilizando `XLNet` e `seu tokenizer`, que inclui diretamente a chamada `generate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on posting about this experience here tomorrow. This is also in the beginning of the month of February, which is my first time back home in January. As the moon rises, my own \"love\" (as I called it to the beginning of the month of February in 2015) (and I think that it probably was in this first month of January in January\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1 :]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 565M/565M [01:08<00:00, 8.28MB/s]  \n",
      "/home/eddygiusepe/24_Transformers_NLP/NLP_Transformers/venv_transformers/lib/python3.8/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFXLNetLMHeadModel.\n",
      "\n",
      "All the layers of TFXLNetLMHeadModel were initialized from the model checkpoint at xlnet-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetLMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on writing for the fourth week of the next four. I was thinking of a picture of me and my mother and grandmother, who we have had for over a week now. There is this picture of my mom and me with my family in the family room in our living room. In one of the pictures I see my family eating breakfast with my mother and grandmother and I.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1 :]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualmente, a gera√ß√£o de texto √© poss√≠vel com `GPT-2`, `OpenAi-GPT`, `CTRL`, `XLNet`, `Transfo-XL` e `Reformer` no `PyTorch` e tamb√©m para a maioria dos modelos no `Tensorflow`. Como pode ser visto no exemplo acima, `XLNet` e `Transfo-XL` geralmente precisam ser preenchidos para funcionar bem. `GPT-2` <font color=\"yellow\">geralmente √© uma boa escolha para gera√ß√£o de texto aberto porque foi treinado em milh√µes de p√°ginas da web com um objetivo de modelagem de linguagem causal.</font>\n",
    "\n",
    "Para obter mais informa√ß√µes sobre como aplicar diferentes estrat√©gias de decodifica√ß√£o para gera√ß√£o de texto, consulte tamb√©m nossa postagem no blog de gera√ß√£o de texto [aqui](https://huggingface.co/blog/how-to-generate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Named Entity Recognition` (NER) √© a tarefa de classificar tokens de acordo com uma classe, </font color=\"yellow\">por exemplo</font>, identificando um token como uma pessoa (`person`), uma organiza√ß√£o ou um local. Um exemplo de Dataset de reconhecimento de entidade nomeada √© o Dataset `CoNLL-2003`, que √© totalmente baseado nessa tarefa. Se voc√™ deseja ajustar (`fine-tune`) um modelo em uma `tarefa NER`, pode aproveitar o script [run_ner.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/run_ner.py).\n",
    "\n",
    "\n",
    "Aqui est√° um exemplo de uso de pipelines para fazer `reconhecimento de entidade nomeada`, especificamente, tentando identificar tokens como pertencentes a uma das $9$ classes:\n",
    "\n",
    "* `O`, Fora de uma entidade nomeada\n",
    "\n",
    "* `B-MIS`, In√≠cio de uma entidade diversa (miscellaneous) logo ap√≥s outra entidade diversa\n",
    "\n",
    "* `I-MIS`, entidade diversa\n",
    "\n",
    "* `B-PER`, in√≠cio do nome de uma pessoa logo ap√≥s o nome de outra pessoa\n",
    "\n",
    "* `I-PER`, nome da pessoa\n",
    "\n",
    "* `B-ORG`, In√≠cio de uma organiza√ß√£o logo ap√≥s outra organiza√ß√£o\n",
    "\n",
    "* `I-ORG`, Organiza√ß√£o\n",
    "\n",
    "* `B-LOC`, In√≠cio de um local logo ap√≥s outro local\n",
    "\n",
    "* `I-LOC`, Localiza√ß√£o\n",
    "\n",
    "Ele aproveita um modelo ajustado no `CoNLL-2003`, ajustado por [@stefan-it](https://github.com/stefan-it) de [dbmdz](https://github.com/dbmdz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipe = pipeline(\"ner\")\n",
    "\n",
    "sequence = \"\"\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO,\n",
    "therefore very close to the Manhattan Bridge which is visible from the window.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso gera uma lista de todas as palavras que foram identificadas como uma das entidades das $9$ classes definidas acima. Aqui est√£o os resultados esperados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-ORG', 'score': 0.99957865, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-ORG', 'score': 0.9909764, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': 0.9982224, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n",
      "{'entity': 'I-ORG', 'score': 0.9994879, 'index': 4, 'word': 'Inc', 'start': 13, 'end': 16}\n",
      "{'entity': 'I-LOC', 'score': 0.9994344, 'index': 11, 'word': 'New', 'start': 40, 'end': 43}\n",
      "{'entity': 'I-LOC', 'score': 0.99931955, 'index': 12, 'word': 'York', 'start': 44, 'end': 48}\n",
      "{'entity': 'I-LOC', 'score': 0.9993794, 'index': 13, 'word': 'City', 'start': 49, 'end': 53}\n",
      "{'entity': 'I-LOC', 'score': 0.98625827, 'index': 19, 'word': 'D', 'start': 79, 'end': 80}\n",
      "{'entity': 'I-LOC', 'score': 0.95142686, 'index': 20, 'word': '##UM', 'start': 80, 'end': 82}\n",
      "{'entity': 'I-LOC', 'score': 0.9336589, 'index': 21, 'word': '##BO', 'start': 82, 'end': 84}\n",
      "{'entity': 'I-LOC', 'score': 0.9761654, 'index': 28, 'word': 'Manhattan', 'start': 114, 'end': 123}\n",
      "{'entity': 'I-LOC', 'score': 0.9914629, 'index': 29, 'word': 'Bridge', 'start': 124, 'end': 130}\n"
     ]
    }
   ],
   "source": [
    "for entity in ner_pipe(sequence):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe como os tokens da sequ√™ncia `\"Hugging Face\"` foram identificados como uma organiza√ß√£o, e `\"New York City\"`, `\"DUMBO\"` e `\"Manhattan Bridge\"` foram identificados como locais.\n",
    "\n",
    "Aqui est√° um exemplo de reconhecimento de entidade nomeada, usando um modelo e um tokenizador. O processo √© o seguinte:\n",
    "\n",
    "\n",
    "<font color=\"orange\">1.</font> Instancie um tokenizador e um modelo a partir do nome do checkpoint. O modelo √© identificado como um `modelo BERT` e o carrega com os pesos armazenados no ponto de verifica√ß√£o.\n",
    "\n",
    "<font color=\"orange\">2.</font> Defina uma sequ√™ncia com entidades conhecidas, como `\"Hugging Face\"` como uma organiza√ß√£o e `\"New York City\"` como um local.\n",
    "\n",
    "<font color=\"orange\">3.</font> Divida as palavras em tokens para que possam ser mapeadas para previs√µes. Usamos um pequeno hack, primeiro, `codificando` e `decodificando` completamente a sequ√™ncia, para que fiquemos com uma string que cont√©m os tokens especiais.\n",
    "\n",
    "<font color=\"orange\">4.</font> Codifique (Encode) essa sequ√™ncia em IDs (`tokens especiais s√£o adicionados automaticamente`).\n",
    "\n",
    "<font color=\"orange\">5.</font> Recupere as previs√µes passando a entrada para o modelo e obtendo a primeira sa√≠da. Isso resulta em uma distribui√ß√£o nas $9$ classes poss√≠veis para cada token. Tomamos o `argmax` para recuperar a classe mais prov√°vel para cada token.\n",
    "\n",
    "<font color=\"orange\">6.</font> Compacte cada token com sua previs√£o e imprima-o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, \"\n",
    "    \"therefore very close to the Manhattan Bridge.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "outputs = model(**inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 8, 8, 8, 0, 0, 0, 0, 0, 8, 8, 8, 0, 0,\n",
       "         0, 0, 0, 0, 8, 8, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.33G/1.33G [11:29<00:00, 1.94MB/s]  \n",
      "2022-11-25 02:29:51.764730: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-25 02:29:51.765373: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (eddygiusepe): /proc/driver/nvidia/version does not exist\n",
      "2022-11-25 02:29:51.790797: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing TFBertForTokenClassification: ['dropout_147']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, \"\n",
    "    \"therefore very close to the Manhattan Bridge.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "outputs = model(**inputs)[0]\n",
    "predictions = tf.argmax(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso gera uma lista de cada token mapeado para sua previs√£o correspondente. Diferentemente do pipeline, aqui cada token tem uma predi√ß√£o, pois n√£o removemos a classe `\"0\"`, o que significa que nenhuma entidade espec√≠fica foi encontrada naquele token.\n",
    "\n",
    "No exemplo acima, `predictions` √© um n√∫mero inteiro que corresponde √† classe predita. Podemos utilizar a propriedade `model.config.id2label` para recuperar o nome da classe correspondente ao n√∫mero da classe, que est√° ilustrado abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'O')\n",
      "('Hu', 'I-ORG')\n",
      "('##gging', 'I-ORG')\n",
      "('Face', 'I-ORG')\n",
      "('Inc', 'I-ORG')\n",
      "('.', 'O')\n",
      "('is', 'O')\n",
      "('a', 'O')\n",
      "('company', 'O')\n",
      "('based', 'O')\n",
      "('in', 'O')\n",
      "('New', 'I-LOC')\n",
      "('York', 'I-LOC')\n",
      "('City', 'I-LOC')\n",
      "('.', 'O')\n",
      "('Its', 'O')\n",
      "('headquarters', 'O')\n",
      "('are', 'O')\n",
      "('in', 'O')\n",
      "('D', 'I-LOC')\n",
      "('##UM', 'I-LOC')\n",
      "('##BO', 'I-LOC')\n",
      "(',', 'O')\n",
      "('therefore', 'O')\n",
      "('very', 'O')\n",
      "('close', 'O')\n",
      "('to', 'O')\n",
      "('the', 'O')\n",
      "('Manhattan', 'I-LOC')\n",
      "('Bridge', 'I-LOC')\n",
      "('.', 'O')\n",
      "('[SEP]', 'O')\n"
     ]
    }
   ],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization (resumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `sumariza√ß√£o` √© a tarefa de resumir um documento ou um artigo em um texto mais curto. Se quiser ajustar um modelo em uma tarefa de resumo, voc√™ pode aproveitar o script [run_summarization.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py).\n",
    "\n",
    "Um exemplo de conjunto de dados de summarization √© o conjunto de dados `CNN/Daily Mail`, que consiste em artigos de not√≠cias longos e foi criado para a tarefa de summarization. Se voc√™ quiser ajustar um modelo em uma tarefa de resumo, v√°rias abordagens s√£o descritas neste [documento](https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/README.md).\n",
    "\n",
    "Aqui est√° um exemplo de uso dos pipelines para fazer o resumo (summarization). Ele aproveita um `modelo de Bart` que foi ajustado no conjunto de dados da `CNN/Daily Mail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.80k/1.80k [00:00<00:00, 962kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_transformers': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "995997e0fa455d01783bd0f7e00bfa7ac3c67ebd3b417bdb15c32058a2eecebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
