{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">A Brief Introduction to BERT</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À medida que aprendemos [o que é um Transformer](https://machinelearningmastery.com/the-transformer-model/) e como podemos [treinar o modelo Transformer](https://machinelearningmastery.com/training-the-transformer-model/), percebemos que é uma ótima ferramenta para fazer um computador entender a **linguagem humana**. No entanto, o `Transformer` foi originalmente projetado como um modelo para traduzir um idioma para outro. Se o redirecionarmos para uma tarefa diferente, provavelmente precisaremos treinar novamente todo o modelo do zero. Dado que o tempo necessário para treinar um modelo de `Transformer` é enorme, gostaríamos de ter uma solução que nos permitisse reutilizar prontamente o Transformer treinado para muitas tarefas diferentes. O `BERT` é um desses modelos. É uma extensão da parte do `encoder` (codificador) de um transformador.\n",
    "\n",
    "Neste script, você aprenderá o que é o `BERT` e descobrirá o que ele pode fazer. Os pontos a estudar são:\n",
    "\n",
    "* O que é um `Bidirectional Encoder Representations de Transformer` (<font color=\"yellow\">BERT</font>)\n",
    "\n",
    "* Como um modelo `BERT` pode ser reutilizado para diferentes propósitos\n",
    "\n",
    "* Como você pode usar um modelo `BERT pré-treinado`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do modelo de Transformer ao BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No modelo do `Transformer`, o Encoder (codificador) e o Decoder (decodificador) são conectados para fazer um `modelo seq2seq` para que você faça uma tradução, como do inglês para o alemão, etc. Lembre-se de que a equação da atenção diz:\n",
    "\n",
    "$$\n",
    "attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V,\n",
    "$$\n",
    "\n",
    "onde $Q$, $K$ e $V$ acima é um vetor EMBEDDING transformado por uma matriz de peso no modelo do Transformer. Treinar um modelo de transformer significa encontrar essas **matrizes de peso**. Uma vez que as matrizes de peso são aprendidas, o transformer se torna um **modelo de linguagem**, o que significa que representa uma maneira de entender a linguagem que você usou para treiná-lo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/162096/3_encoder_decoder_class.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um transformer possui partes codificadoras (`Encoder`) e decodificadoras (`Decoder`). Como o nome indica, o <font color=\"yellow\">codificador</font> transforma frases e parágrafos em um formato interno (`uma matriz numérica`) que entende o contexto, enquanto o <font color=\"yellow\">decodificador</font> faz o inverso. A combinação do codificador e do decodificador permite que um transformer execute tarefas `seq2seq`, como `tradução`. Se você retirar a parte do codificador do Transformer, ela pode lhe dizer algo sobre o contexto, o que pode fazer algo interessante.\n",
    "\n",
    "A `representação do codificador bidirecional do Transformer` (BERT) aproveita o modelo de atenção (Attention Model) para obter uma compreensão mais profunda do contexto da linguagem. <font color=\"orange\">`BERT` é uma pilha de muitos blocos codificadores (Encoder)</font>. O texto de entrada é separado em tokens como no modelo do Transformer, e cada token será transformado em um vetor na saída do BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que o BERT pode fazer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um modelo `BERT` é treinado usando o <font color=\"orange\">modelo de linguagem mascarada (MLM)</font> e <font color=\"yellow\">a previsão da próxima sentença (NSP)</font> simultaneamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://machinelearningmastery.com/wp-content/uploads/2022/10/BERT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada amostra de treinamento para `BERT` é um par de frases (sentences) de um documento. As duas frases podem ser consecutivas no documento ou não. Haverá um token `[CLS]` anexado à primeira frase (para representar a **classe**) e um token `[SEP]` anexado a cada frase (como um **separador**). Em seguida, as duas sentenças serão concatenadas como uma sequência de tokens para se tornar uma amostra de treinamento. Uma pequena porcentagem dos tokens na amostra de treinamento é `mascarada` (*masked*) com um token especial `[MASK]` ou substituída por um token aleatório.\n",
    "\n",
    "Antes de ser alimentado no modelo `BERT`, os tokens na amostra de treinamento serão transformados em `vetores EMBEDDINGS`, com as codificações (encodings) posicionais adicionadas, e particulares ao `BERT`, com `segmentos de Embeddings` adicionados, bem como para marcar se o token é da primeira ou da segunda frase.\n",
    "\n",
    "\n",
    "Cada token de entrada para o modelo `BERT` produzirá um vetor de saída. Em um modelo BERT bem treinado, esperamos:\n",
    "\n",
    "* <font color=\"orange\">saída correspondente ao token mascarado pode revelar qual era o token original</font>\n",
    "\n",
    "* <font color=\"orange\">a saída correspondente ao token `[CLS]` no início pode revelar se as duas sentenças são consecutivas no documento</font>\n",
    "\n",
    "\n",
    "Então, os pesos treinados no modelo `BERT` podem entender bem o contexto da linguagem.\n",
    "\n",
    "\n",
    "Depois de ter esse modelo de BERT, você pode usá-lo para muitas `tarefas de downstream`. <font color=\"yellow\">Por exemplo</font>, adicionando uma camada de `classificação` apropriada em cima de um codificador (`Encoder`) e alimentando apenas uma frase ao modelo em vez de um par, você pode usar o token de classe `[CLS]` como entrada para `classificação de sentimento`. Funciona porque a saída do token de classe é treinada para agregar a atenção para toda a entrada.\n",
    "\n",
    "Outro <font color=\"yellow\">exemplo</font> é tomar uma pergunta como a primeira frase e o texto (por exemplo, um parágrafo) como a segunda frase, então o token de saída da segunda frase pode marcar a posição onde a resposta à pergunta estava. Funciona porque a saída de cada token revela algumas informações sobre esse token no contexto de toda a entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
